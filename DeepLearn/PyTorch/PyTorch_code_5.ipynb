{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-05T04:30:49.652664Z",
     "start_time": "2025-06-05T04:30:45.133724Z"
    }
   },
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# accelerator 加速装置\n",
    "# available 可利用的\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Using {device} device\")\n",
    "print(torch.accelerator.is_available())\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "False\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "验证设备是否存在加速装置，如果存在就利用他否则则使用CPU。",
   "id": "5a1b1817958d6137"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T04:30:54.232436Z",
     "start_time": "2025-06-05T04:30:54.195373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# neural 神经的\n",
    "# network 网络\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()  # 使平坦\n",
    "        self.linear_relu_stack = nn.Sequential(  # 直线的， 激活函数， 堆叠， 按次序的\n",
    "            nn.Linear(28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n"
   ],
   "id": "7b9f8132d113c4b0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T04:30:57.518699Z",
     "start_time": "2025-06-05T04:30:57.443499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")\n"
   ],
   "id": "5aec7005828dac01",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([4])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "将图片张量输入模型，模型输出一个二维张量。每一行表示一个批次内不同的样本，每一列表示不同的类别。从每一行中选出概率最大的那个类别。也就是模型的预测结果。",
   "id": "1a6a4783d6c9f083"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T04:31:15.668729Z",
     "start_time": "2025-06-05T04:31:15.643599Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 模型曾\n",
    "\n",
    "input_image = torch.rand(3, 28, 28)\n",
    "print(input_image.size())\n"
   ],
   "id": "c6d9229d65d6e598",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T04:31:17.631645Z",
     "start_time": "2025-06-05T04:31:17.620106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 利用nn.Flatten扁平化展开除每一批次的样本的张量。\n",
    "\n",
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())  # 展开后的形状 [3, 748]\n",
    "\n",
    "print(\"-\" * 70)\n",
    "\n",
    "tensor = torch.tensor([\n",
    "    [\n",
    "        [\n",
    "            [0, 0, 255],\n",
    "            [0, 255, 255],\n",
    "            [255, 0, 255],\n",
    "        ],\n",
    "        [\n",
    "            [0, 255, 0],\n",
    "            [255, 0, 255],\n",
    "            [255, 255, 255]\n",
    "        ],\n",
    "        [\n",
    "            [1, 5, 5],\n",
    "            [3, 4, 7],\n",
    "            [7, 8, 10]\n",
    "        ]\n",
    "    ],\n",
    "    [\n",
    "        [\n",
    "            [255, 0, 100],\n",
    "            [150, 255, 190],\n",
    "            [100, 30, 255],\n",
    "        ],\n",
    "        [\n",
    "            [2, 4, 5],\n",
    "            [255, 150, 155],\n",
    "            [190, 155, 255]\n",
    "        ],\n",
    "        [\n",
    "            [2, 4, 7],\n",
    "            [4, 7, 8],\n",
    "            [255, 250, 200]\n",
    "        ]\n",
    "    ],\n",
    "    [\n",
    "        [\n",
    "            [12, 155, 255],\n",
    "            [255, 255, 255],\n",
    "            [150, 150, 70]\n",
    "        ],\n",
    "        [\n",
    "            [240, 255, 100],\n",
    "            [10, 20, 30],\n",
    "            [255, 255, 255]\n",
    "        ],\n",
    "        [\n",
    "            [255, 255, 255],\n",
    "            [250, 250, 250],\n",
    "            [150, 250, 245]\n",
    "        ]\n",
    "    ]\n",
    "])\n",
    "\n",
    "print(tensor.size())\n",
    "flat_tensor = flatten(tensor)\n",
    "print(flat_tensor.size())\n",
    "print()\n",
    "print(flat_tensor)\n",
    "\n"
   ],
   "id": "1f4e6d16037fbac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n",
      "----------------------------------------------------------------------\n",
      "torch.Size([3, 3, 3, 3])\n",
      "torch.Size([3, 27])\n",
      "\n",
      "tensor([[  0,   0, 255,   0, 255, 255, 255,   0, 255,   0, 255,   0, 255,   0,\n",
      "         255, 255, 255, 255,   1,   5,   5,   3,   4,   7,   7,   8,  10],\n",
      "        [255,   0, 100, 150, 255, 190, 100,  30, 255,   2,   4,   5, 255, 150,\n",
      "         155, 190, 155, 255,   2,   4,   7,   4,   7,   8, 255, 250, 200],\n",
      "        [ 12, 155, 255, 255, 255, 255, 150, 150,  70, 240, 255, 100,  10,  20,\n",
      "          30, 255, 255, 255, 255, 255, 255, 250, 250, 250, 150, 250, 245]])\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "通过Flatten将高维的张量进行展平，但是批次通道不变。\n",
    "方法： from torch import nn flatten = nn.Flatten(), 然后输入图片张量进行展开 flat_image = flatten(input_image)."
   ],
   "id": "f0c23dbe13a4fbd4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T04:31:21.747869Z",
     "start_time": "2025-06-05T04:31:21.727141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# nn.Linear  线性\n",
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())\n"
   ],
   "id": "8b9372e4fcdc196d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "对展开的张量进行线性变换。in_features 和 out_features 参数分别代表输入全连接层的元素数量，也就是展开之后每个样本的一维张量里的元素数量。",
   "id": "60bbccb4a7ea83e8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T04:31:24.637329Z",
     "start_time": "2025-06-05T04:31:24.618463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# nn.ReLU  激活函数\n",
    "\n",
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")\n",
    "\n"
   ],
   "id": "419981026a9fed59",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[-0.5035, -0.2911, -0.0655,  0.1264,  0.1276,  0.2771,  0.4132,  0.1046,\n",
      "          0.1311, -0.2914, -0.3006, -0.4146,  0.0043, -0.0407,  1.1606,  0.6067,\n",
      "          0.3848, -0.1609, -0.0526, -0.1813],\n",
      "        [-0.3996, -0.4265, -0.1083, -0.0391,  0.1665,  0.4641,  0.4647,  0.1719,\n",
      "          0.4001, -0.6122, -0.2807, -0.2160, -0.2635,  0.0955,  0.8089,  0.6653,\n",
      "          0.5401, -0.0525, -0.1033,  0.0031],\n",
      "        [-0.2210, -0.4284, -0.1172,  0.5052, -0.2690,  0.4579,  0.4986, -0.0300,\n",
      "          0.1495, -0.5997, -0.4747, -0.2359, -0.1417, -0.0040,  0.6733,  0.5509,\n",
      "          0.5146, -0.2834,  0.1559,  0.1633]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.0000, 0.0000, 0.0000, 0.1264, 0.1276, 0.2771, 0.4132, 0.1046, 0.1311,\n",
      "         0.0000, 0.0000, 0.0000, 0.0043, 0.0000, 1.1606, 0.6067, 0.3848, 0.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1665, 0.4641, 0.4647, 0.1719, 0.4001,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0955, 0.8089, 0.6653, 0.5401, 0.0000,\n",
      "         0.0000, 0.0031],\n",
      "        [0.0000, 0.0000, 0.0000, 0.5052, 0.0000, 0.4579, 0.4986, 0.0000, 0.1495,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6733, 0.5509, 0.5146, 0.0000,\n",
      "         0.1559, 0.1633]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "通过激活函数引入非线性映射，让神经网络学习到各种各样通用的特征，而非记住参数和顺序。\n",
    "方法： out = nn.ReLU(Linear_in)"
   ],
   "id": "2e15ad1590df2ee3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T04:42:30.678573Z",
     "start_time": "2025-06-05T04:42:30.672105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# nn.Sequential 使用这个容器来容纳所有的神经网络层\n",
    "\n",
    "seq_modules = nn.Sequential(\n",
    "     flatten,\n",
    "     layer1,\n",
    "     nn.ReLU(),\n",
    "     nn.Linear(20, 10),\n",
    " )\n",
    "\n",
    "input_image = torch.rand(3, 28, 28)\n",
    "logits = seq_modules(input_image)\n",
    "\n",
    "print(logits)\n",
    "\n"
   ],
   "id": "65ac127c648a7127",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0092, 0.0000, 0.0000, 0.0000, 0.1112,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0274, 0.0000, 0.0344, 0.0000, 0.0000, 0.1071, 0.0679, 0.0921,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0308, 0.0402, 0.0000, 0.0000, 0.1293, 0.0195, 0.0195,\n",
      "         0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "创建一个容器Sequential来容纳神经网络。\n",
    "方法： seq_modules = nn.Sequential(flatten, nn.Linear(), nn.ReLU)."
   ],
   "id": "631503d35da59e53"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T04:53:52.669858Z",
     "start_time": "2025-06-05T04:53:52.655698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)\n",
    "\n",
    "print(pred_probab)\n",
    "\n",
    "\n"
   ],
   "id": "b0987a6ac3164d16",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0987, 0.0987, 0.0987, 0.0987, 0.0997, 0.0987, 0.0987, 0.0987, 0.1104,\n",
      "         0.0987],\n",
      "        [0.0967, 0.0994, 0.0967, 0.1001, 0.0967, 0.0967, 0.1076, 0.1035, 0.1060,\n",
      "         0.0967],\n",
      "        [0.0976, 0.0976, 0.1006, 0.1016, 0.0976, 0.0976, 0.1110, 0.0995, 0.0995,\n",
      "         0.0976]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T05:13:20.934426Z",
     "start_time": "2025-06-05T05:13:20.923366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 模型参数\n",
    "\n",
    "print(f\"Model: {model}\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer Name: {name} | Layer Size: {param.size()} | Layer Value: {param[:2]}\")\n",
    "\n",
    "\n"
   ],
   "id": "620abcdfb255bd4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Layer Name: linear_relu_stack.0.weight | Layer Size: torch.Size([512, 784]) | Layer Value: tensor([[ 0.0258, -0.0059, -0.0350,  ...,  0.0020, -0.0166, -0.0305],\n",
      "        [-0.0160, -0.0071, -0.0236,  ...,  0.0352, -0.0013,  0.0078]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Layer Name: linear_relu_stack.0.bias | Layer Size: torch.Size([512]) | Layer Value: tensor([-0.0351,  0.0240], grad_fn=<SliceBackward0>)\n",
      "Layer Name: linear_relu_stack.2.weight | Layer Size: torch.Size([512, 512]) | Layer Value: tensor([[ 0.0166,  0.0267, -0.0161,  ...,  0.0299,  0.0331, -0.0242],\n",
      "        [ 0.0429, -0.0036, -0.0140,  ...,  0.0223, -0.0234,  0.0043]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Layer Name: linear_relu_stack.2.bias | Layer Size: torch.Size([512]) | Layer Value: tensor([-0.0148, -0.0371], grad_fn=<SliceBackward0>)\n",
      "Layer Name: linear_relu_stack.4.weight | Layer Size: torch.Size([10, 512]) | Layer Value: tensor([[ 0.0146,  0.0233,  0.0366,  ...,  0.0199,  0.0266,  0.0113],\n",
      "        [ 0.0351, -0.0072,  0.0113,  ...,  0.0419, -0.0110,  0.0027]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Layer Name: linear_relu_stack.4.bias | Layer Size: torch.Size([10]) | Layer Value: tensor([0.0135, 0.0069], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "通过 model.named_parameters() 方法来打印需要调整优化参数层的参数。ReLu层的参数不会被打印，因为映射的关系已经确定，不需要再去优化参数。\n",
    "打印参数时注意切片，不然大矩阵会让打印信息很难观察。"
   ],
   "id": "c89da52d6d165716"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
