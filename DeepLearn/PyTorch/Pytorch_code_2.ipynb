{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-28T08:56:42.567783Z",
     "start_time": "2025-05-28T08:56:40.325421Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "张量的创建以及初始化。\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "demo = [[1, 2], [2, 3], [2, 5], [0, 29]]\n",
    "\n",
    "array = np.array(demo)\n",
    "\n",
    "print(f\"通过numpy，将 {type(demo)} --> {type(array)}\")\n",
    "\n",
    "tensor_ = torch.tensor(demo)\n",
    "_tensor = torch.from_numpy(array)\n",
    "\n",
    "print(f\"通过torch.tensor来实现 {type(demo)} --> {type(tensor_)}\")\n",
    "print(f\"通过torch.from_numpy来实现 {type(array)} --> {type(_tensor)}\")\n",
    "\n",
    "\n",
    "# 随机张量\n",
    "shape = (5, 5,)  # 张量的形状\n",
    "\n",
    "tensor1 = torch.ones(shape)\n",
    "print(f\"这是随机生成的由一为元素构成的张量：\\n {tensor1}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "tensor2 = torch.zeros(shape)\n",
    "print(f\"这是随机生成的由零为元素构成的张量: \\n {tensor2}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "tensor3 = torch.rand(shape)\n",
    "print(f\"这是由零到一之间的随机数为元素生成的张量： \\n {tensor3}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-\" * 70)\n",
    "print(\"在原有的张量基础上，创建一个具有相同形状和设备信息的张量：\")\n",
    "\n",
    "\n",
    "# 对原有的张量使用1,0,或者随机张量进行覆盖修改。\n",
    "list_ = [[1, 2, 3], [2, 6, 7], [3, 4, 6], [5, 6, 10]]\n",
    "array_ = np.array(list_)\n",
    "tensor_ = torch.from_numpy(array_)\n",
    "print(f\"--> List: \\n{list_}\")\n",
    "print(f\"--> Array: \\n{array_}\")\n",
    "print(f\"--> Tensor: \\n{tensor_}\")\n",
    "print(\"<\" + \"-\" * 70 + \">\")\n",
    "tensor_1 = torch.zeros_like(tensor_)\n",
    "print(f\"将原张量的元素全部变为0： \\n{tensor_1}\")\n",
    "print(\"-\" * 70)\n",
    "tensor_2 = torch.ones_like(tensor_)\n",
    "print(f\"将原张量的元素全部变为1： \\n{tensor_2}\")\n",
    "print(\"-\" * 70)\n",
    "tensor_3 = torch.rand_like(tensor_, dtype=torch.float)\n",
    "print(f\"将原张量的元素变为0-1之间的随机数： \\n{tensor_3}\")\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "通过numpy，将 <class 'list'> --> <class 'numpy.ndarray'>\n",
      "通过torch.tensor来实现 <class 'list'> --> <class 'torch.Tensor'>\n",
      "通过torch.from_numpy来实现 <class 'numpy.ndarray'> --> <class 'torch.Tensor'>\n",
      "这是随机生成的由一为元素构成的张量：\n",
      " tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "--------------------------------------------------\n",
      "这是随机生成的由零为元素构成的张量: \n",
      " tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "--------------------------------------------------\n",
      "这是由零到一之间的随机数为元素生成的张量： \n",
      " tensor([[0.5374, 0.3768, 0.6241, 0.4075, 0.4195],\n",
      "        [0.2235, 0.0712, 0.6312, 0.7126, 0.7894],\n",
      "        [0.3703, 0.7517, 0.5220, 0.0088, 0.7283],\n",
      "        [0.6338, 0.0036, 0.3246, 0.5362, 0.0519],\n",
      "        [0.3004, 0.0851, 0.2528, 0.1090, 0.1261]])\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "在原有的张量基础上，创建一个具有相同形状和设备信息的张量：\n",
      "--> List: \n",
      "[[1, 2, 3], [2, 6, 7], [3, 4, 6], [5, 6, 10]]\n",
      "--> Array: \n",
      "[[ 1  2  3]\n",
      " [ 2  6  7]\n",
      " [ 3  4  6]\n",
      " [ 5  6 10]]\n",
      "--> Tensor: \n",
      "tensor([[ 1,  2,  3],\n",
      "        [ 2,  6,  7],\n",
      "        [ 3,  4,  6],\n",
      "        [ 5,  6, 10]])\n",
      "<---------------------------------------------------------------------->\n",
      "将原张量的元素全部变为0： \n",
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n",
      "----------------------------------------------------------------------\n",
      "将原张量的元素全部变为1： \n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1]])\n",
      "----------------------------------------------------------------------\n",
      "将原张量的元素变为0-1之间的随机数： \n",
      "tensor([[0.8086, 0.5098, 0.6579],\n",
      "        [0.9419, 0.6105, 0.5865],\n",
      "        [0.5110, 0.2509, 0.9020],\n",
      "        [0.0568, 0.9662, 0.6008]])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "上面的这个代码用于张量的初始化和覆盖已有张量：\n",
    "A.转换张量：\n",
    "    1.对于非列表的数据结构：\n",
    "      a.先将其转换成列表，然后再根据后续的步骤操作进行处理。\n",
    "    2.对于列表的数据结构：\n",
    "        a.直接将其转换成张量：\n",
    "          tensor = torch.tensor(list)\n",
    "        b.先将其转换成numpy数组，再将其转换成张量：\n",
    "          array = np.array(list)\n",
    "          tensor = torch.from_numpy(array)\n",
    "    3.对于numpy数组的数据结构：\n",
    "        a.直接将其转换成tensor：\n",
    "          tensor = torch.from_numpy(array)\n",
    "\n",
    "B.随机张量：\n",
    "    <->.首先定义张量的形状：\n",
    "        shape = (x, y,)\n",
    "    1.随机元素为0的张量：\n",
    "      tensor = torch.zeros(shape)\n",
    "    2.随机元素为[0, 1)之间的浮点数：\n",
    "      tensor = torch.rand(shape)\n",
    "    3.随机元素为1的张量：\n",
    "      tensor = torch.ones(shape)\n",
    "\n",
    "C.修改覆盖张量：\n",
    "    <->.修改后的张量形状和设备信息不会发送改变。\n",
    "    1.将原张量元素全部修改为0：\n",
    "      new_tensor = torch.zeros_like(base_tensor)\n",
    "    2.将原张量元素全部修改为[0,1)之间的浮点数：\n",
    "      new_tensor = torch.rand_like(base_tensor, dtype=torch.gloat)  # 必须显式指定数据类型为浮点数。\n",
    "    3.将原张量元素全部修改为1：\n",
    "      new_tensor = torch.ones_like(base_tensor)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "e7ad033d5168f02"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T08:57:23.135319Z",
     "start_time": "2025-05-28T08:57:23.120540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "张量的属性\n",
    "\"\"\"\n",
    "import torch\n",
    "\n",
    "\n",
    "# 形状，数据类型，设备信息。\n",
    "tensor = torch.rand(3, 4)\n",
    "tensor_shape = tensor.shape\n",
    "tensor_dtype = tensor.dtype\n",
    "tensor_device = tensor.device\n",
    "print(f\"这个随机生成的3X4张量，它的形状是 {tensor_shape} 数据类型是 {tensor_dtype} 设备信息是 {tensor_device}\")\n"
   ],
   "id": "f8ba89c4fd3392af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这个随机生成的3X4张量，它的形状是 torch.Size([3, 4]) 数据类型是 torch.float32 设备信息是 cpu\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "这里通过张量的三个属性来获取张量的信息，这三个属性分别为shape，dtype，device。",
   "id": "c50c2dad2cc3c93a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T08:57:13.879191Z",
     "start_time": "2025-05-28T08:57:13.865734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "张量转移\n",
    "\"\"\"\n",
    "import torch\n",
    "\n",
    "\n",
    "shape = (3, 3)\n",
    "tensor = torch.rand(shape, dtype=torch.float, device=\"cpu\")\n",
    "\n",
    "if torch.accelerator.is_available():\n",
    "    tensor = tensor.to(torch.accelerator.current_accelerator())\n",
    "    print(f\"当前张量位于： {tensor.device}\")\n",
    "else:\n",
    "    print(\"加速不可用！\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "21d7e888652196ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加速不可用！\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "此处判断硬件上是否存在加速设备，如果存在就将张量转移到加速设备上。",
   "id": "f13dff56b3cabcd3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T10:22:53.155402Z",
     "start_time": "2025-05-28T10:22:53.083834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "张量的索引和切片\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "# 定义一个列表\n",
    "lt = [\n",
    "    [1, 1, 2, 3, 4, 5, 6],\n",
    "    [2, 1, 2, 3, 4, 5, 6],\n",
    "    [3, 1, 2, 3, 4, 5, 6],\n",
    "    [4, 1, 2, 3, 4, 5, 6],\n",
    "    [5, 1, 2, 3, 4, 5, 6],\n",
    "    [6, 1, 2, 3, 4, 5, 6]\n",
    "]\n",
    "# 将列表转换成张量,这个张量是6 * 7的形状。按照数字的增长顺序来定义，方便观察张量的切片和索引操作。\n",
    "tr = torch.tensor(lt)\n",
    "# 打印这个张量\n",
    "print(tr)\n",
    "\n",
    "# 打印第一行和最一行\n",
    "print(tr[0])\n",
    "print(tr[-1])\n",
    "\n",
    "# 打印从第二行到倒数第二行\n",
    "print(tr[1:-1])\n",
    "\n",
    "# 修改第一行的张量\n",
    "tr[0, :] = torch.tensor([5, 2, 0, 1, 3, 4, 8])\n",
    "print(tr)\n",
    "\n",
    "# 打印张量的第一列\n",
    "print(tr[:, 0])\n",
    "\n",
    "# 打印张量的最后一列\n",
    "print(tr[:, -1])\n",
    "print(tr[..., -1])\n",
    "\n",
    "# 打印中间多列\n",
    "print(tr[..., 0:-1])\n",
    "\n",
    "# 修改第一列的张量\n",
    "tr[:, 0] = 10\n",
    "print(tr)\n",
    "\n",
    "# 打印第一行，第一列到第二列的张量。\n",
    "print(tr[0:1, 0:3])\n",
    "\n",
    "\n",
    "print(\"------------------分割线---------------------\")\n",
    "\n",
    "\n",
    "# 创建一个三维张量，从列表开始自定义。\n",
    "li = [\n",
    "    [\n",
    "        [1, 2],\n",
    "        [1, 2],\n",
    "        [1, 2]\n",
    "    ],\n",
    "    [\n",
    "        [1, 2],\n",
    "        [1, 2],\n",
    "        [1, 2]\n",
    "    ],\n",
    "    [\n",
    "        [1, 2],\n",
    "        [1, 2],\n",
    "        [1, 2]\n",
    "    ]\n",
    "]\n",
    "\n",
    "te = torch.tensor(li)\n",
    "# 打印这个三维张量\n",
    "print(te)\n",
    "# 降维取出\n",
    "print(te[..., 0])\n",
    "\n",
    "print(te[0:2, 0:2, 0])\n",
    "\n",
    "\n",
    "print(\"-----------------------分割线---------------------------\")\n",
    "# 对于后面为...的情况出现在高维张量中.\n",
    "\n",
    "list__ = [\n",
    "    [\n",
    "        [1, 2, 3, 4, 5, 6],\n",
    "        [1, 2, 3, 4, 5, 6],\n",
    "        [1, 2, 3, 4, 5, 6],\n",
    "        [1, 2, 3, 4, 5, 6],\n",
    "        [1, 2, 3, 4, 5, 6],\n",
    "        [1, 2, 3, 4, 5, 6]\n",
    "    ],\n",
    "    [\n",
    "        [1, 2, 3, 4, 5, 6],\n",
    "        [1, 2, 3, 4, 5, 6],\n",
    "        [1, 2, 3, 4, 5, 6],\n",
    "        [1, 2, 3, 4, 5, 6],\n",
    "        [1, 2, 3, 4, 5, 6],\n",
    "        [1, 2, 3, 4, 5, 6]\n",
    "    ],\n",
    "    [\n",
    "        [1, 2, 3, 4, 5, 6],\n",
    "        [1, 2, 3, 4, 5, 6],\n",
    "        [1, 2, 3, 4, 5, 6],\n",
    "        [1, 2, 3, 4, 5, 6],\n",
    "        [1, 2, 3, 4, 5, 6],\n",
    "        [1, 2, 3, 4, 5, 6]\n",
    "    ]\n",
    "]\n",
    "\n",
    "\n",
    "tensor__ = torch.tensor(list__)\n",
    "\n",
    "print(tensor__)\n",
    "\n",
    "print(tensor__[0:2, ...])\n",
    "\n",
    "print(tensor__[0])\n",
    "\n",
    "a = torch.ones(1, 4)\n",
    "print(a)\n",
    "print(a[0: 3])\n",
    "b = torch.tensor([1, 2, 3, 4])\n",
    "print(b[0:2])\n",
    "print([1, 2, 3, 4][0:2])"
   ],
   "id": "db0fb6ecd2e3323e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 2, 3, 4, 5, 6],\n",
      "        [2, 1, 2, 3, 4, 5, 6],\n",
      "        [3, 1, 2, 3, 4, 5, 6],\n",
      "        [4, 1, 2, 3, 4, 5, 6],\n",
      "        [5, 1, 2, 3, 4, 5, 6],\n",
      "        [6, 1, 2, 3, 4, 5, 6]])\n",
      "tensor([1, 1, 2, 3, 4, 5, 6])\n",
      "tensor([6, 1, 2, 3, 4, 5, 6])\n",
      "tensor([[2, 1, 2, 3, 4, 5, 6],\n",
      "        [3, 1, 2, 3, 4, 5, 6],\n",
      "        [4, 1, 2, 3, 4, 5, 6],\n",
      "        [5, 1, 2, 3, 4, 5, 6]])\n",
      "tensor([[5, 2, 0, 1, 3, 4, 8],\n",
      "        [2, 1, 2, 3, 4, 5, 6],\n",
      "        [3, 1, 2, 3, 4, 5, 6],\n",
      "        [4, 1, 2, 3, 4, 5, 6],\n",
      "        [5, 1, 2, 3, 4, 5, 6],\n",
      "        [6, 1, 2, 3, 4, 5, 6]])\n",
      "tensor([5, 2, 3, 4, 5, 6])\n",
      "tensor([8, 6, 6, 6, 6, 6])\n",
      "tensor([8, 6, 6, 6, 6, 6])\n",
      "tensor([[5, 2, 0, 1, 3, 4],\n",
      "        [2, 1, 2, 3, 4, 5],\n",
      "        [3, 1, 2, 3, 4, 5],\n",
      "        [4, 1, 2, 3, 4, 5],\n",
      "        [5, 1, 2, 3, 4, 5],\n",
      "        [6, 1, 2, 3, 4, 5]])\n",
      "tensor([[10,  2,  0,  1,  3,  4,  8],\n",
      "        [10,  1,  2,  3,  4,  5,  6],\n",
      "        [10,  1,  2,  3,  4,  5,  6],\n",
      "        [10,  1,  2,  3,  4,  5,  6],\n",
      "        [10,  1,  2,  3,  4,  5,  6],\n",
      "        [10,  1,  2,  3,  4,  5,  6]])\n",
      "tensor([[10,  2,  0]])\n",
      "------------------分割线---------------------\n",
      "tensor([[[1, 2],\n",
      "         [1, 2],\n",
      "         [1, 2]],\n",
      "\n",
      "        [[1, 2],\n",
      "         [1, 2],\n",
      "         [1, 2]],\n",
      "\n",
      "        [[1, 2],\n",
      "         [1, 2],\n",
      "         [1, 2]]])\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1]])\n",
      "tensor([[1, 1],\n",
      "        [1, 1]])\n",
      "-----------------------分割线---------------------------\n",
      "tensor([[[1, 2, 3, 4, 5, 6],\n",
      "         [1, 2, 3, 4, 5, 6],\n",
      "         [1, 2, 3, 4, 5, 6],\n",
      "         [1, 2, 3, 4, 5, 6],\n",
      "         [1, 2, 3, 4, 5, 6],\n",
      "         [1, 2, 3, 4, 5, 6]],\n",
      "\n",
      "        [[1, 2, 3, 4, 5, 6],\n",
      "         [1, 2, 3, 4, 5, 6],\n",
      "         [1, 2, 3, 4, 5, 6],\n",
      "         [1, 2, 3, 4, 5, 6],\n",
      "         [1, 2, 3, 4, 5, 6],\n",
      "         [1, 2, 3, 4, 5, 6]],\n",
      "\n",
      "        [[1, 2, 3, 4, 5, 6],\n",
      "         [1, 2, 3, 4, 5, 6],\n",
      "         [1, 2, 3, 4, 5, 6],\n",
      "         [1, 2, 3, 4, 5, 6],\n",
      "         [1, 2, 3, 4, 5, 6],\n",
      "         [1, 2, 3, 4, 5, 6]]])\n",
      "tensor([[[1, 2, 3, 4, 5, 6],\n",
      "         [1, 2, 3, 4, 5, 6],\n",
      "         [1, 2, 3, 4, 5, 6],\n",
      "         [1, 2, 3, 4, 5, 6],\n",
      "         [1, 2, 3, 4, 5, 6],\n",
      "         [1, 2, 3, 4, 5, 6]],\n",
      "\n",
      "        [[1, 2, 3, 4, 5, 6],\n",
      "         [1, 2, 3, 4, 5, 6],\n",
      "         [1, 2, 3, 4, 5, 6],\n",
      "         [1, 2, 3, 4, 5, 6],\n",
      "         [1, 2, 3, 4, 5, 6],\n",
      "         [1, 2, 3, 4, 5, 6]]])\n",
      "tensor([[1, 2, 3, 4, 5, 6],\n",
      "        [1, 2, 3, 4, 5, 6],\n",
      "        [1, 2, 3, 4, 5, 6],\n",
      "        [1, 2, 3, 4, 5, 6],\n",
      "        [1, 2, 3, 4, 5, 6],\n",
      "        [1, 2, 3, 4, 5, 6]])\n",
      "tensor([[1., 1., 1., 1.]])\n",
      "tensor([[1., 1., 1., 1.]])\n",
      "tensor([1, 2])\n",
      "[1, 2]\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "张量可以进行类似于numpy数组的索引和切片操作，还可以对其内容进行更新和修改。基本的格式就是类似于列表的索引操作那样，只不过张量可以横竖多个方向，在不同的层面之间用逗号隔开。每个逗号间隔间都可以使用a：b这种操作来对每个不同的层面进行选择。如果只有一个层面，那默认总行开始。张量还可以进行更新和修改，只是修改的内容要和选中的内容形状相同，数据格式相同。：表示全选，而...表示多个连续的:。对于切片出来的张量会比原来的张量低一个维度，如果一个维度只有一个数那可省略不写。对于一维的张量和列表十分类似。\n",
   "id": "7b92856fcade3a56"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T10:25:57.583369Z",
     "start_time": "2025-06-01T10:25:56.355752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "连接张量\n",
    "\"\"\"\n",
    "import torch\n",
    "\n",
    "list1 = [1, 1, 1,1]\n",
    "list2 = [2, 2, 2, 2]\n",
    "list3 = [3, 3, 3, 3]\n",
    "\n",
    "ten1 = torch.tensor(list1)\n",
    "ten2 = torch.tensor(list2)\n",
    "ten3 = torch.tensor(list3)\n",
    "\n",
    "print(f\"Tensor1 is :\\n{ten1}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"Tensor2 is: \\n{ten2}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"Tensor3 is: \\n{ten3}\")\n",
    "print(\"-\" * 70)\n",
    "L = torch.cat([ten1, ten2, ten3], dim=0)\n",
    "print(L)\n",
    "print(\"-\" * 70)\n",
    "\n",
    "list_ = [\n",
    "    [1, 2, 3],\n",
    "    [1, 2, 3],\n",
    "    [1, 2, 3]\n",
    "]\n",
    "\n",
    "_list = [\n",
    "    [4, 5, 6],\n",
    "    [4, 5, 6],\n",
    "    [4, 5, 6]\n",
    "]\n",
    "\n",
    "_list_ = [\n",
    "    [7, 8, 9],\n",
    "    [7, 8, 9],\n",
    "    [7, 8, 9]\n",
    "]\n",
    "\n",
    "tensor_ = torch.tensor(list_)\n",
    "_tensor = torch.tensor(_list)\n",
    "_tensor_ = torch.tensor(_list_)\n",
    "\n",
    "l = torch.cat([tensor_, _tensor, _tensor_], dim=1)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(tensor_)\n",
    "print(\"-\" * 70)\n",
    "print(_tensor)\n",
    "print(\"-\" * 70)\n",
    "print(_tensor_)\n",
    "print(\"-\" * 70)\n",
    "print(l)\n"
   ],
   "id": "1e18c484ccf0f3e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor1 is :\n",
      "tensor([1, 1, 1, 1])\n",
      "----------------------------------------------------------------------\n",
      "Tensor2 is: \n",
      "tensor([2, 2, 2, 2])\n",
      "----------------------------------------------------------------------\n",
      "Tensor3 is: \n",
      "tensor([3, 3, 3, 3])\n",
      "----------------------------------------------------------------------\n",
      "tensor([1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3])\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "tensor([[1, 2, 3],\n",
      "        [1, 2, 3],\n",
      "        [1, 2, 3]])\n",
      "----------------------------------------------------------------------\n",
      "tensor([[4, 5, 6],\n",
      "        [4, 5, 6],\n",
      "        [4, 5, 6]])\n",
      "----------------------------------------------------------------------\n",
      "tensor([[7, 8, 9],\n",
      "        [7, 8, 9],\n",
      "        [7, 8, 9]])\n",
      "----------------------------------------------------------------------\n",
      "tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
      "        [1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
      "        [1, 2, 3, 4, 5, 6, 7, 8, 9]])\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "相同形状的张量之间可以联结，也就是在相同的维度上，把其他张量的数据从后面加到原来的张量上。类似于在同一维度上的两个列表相加，但是值得注意的是链结完，张量的维度是依然不会变的。方法： Cat = torch.cat([tensor1, tensor2, tensor3], dim=维度)",
   "id": "1693ae0a3d505802"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T11:32:51.476063Z",
     "start_time": "2025-06-01T11:32:51.397854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\"\"\"\n",
    "张量的算术运算\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "l_1 = [\n",
    "    [1, 2, 3],\n",
    "    [1, 2, 3],\n",
    "    [1, 2, 3],\n",
    "]\n",
    "\n",
    "l_2 = [\n",
    "    [4, 5, 6],\n",
    "    [4, 5, 6],\n",
    "    [4, 5, 6]\n",
    "]\n",
    "\n",
    "l_3 = [\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0]\n",
    "]\n",
    "\n",
    "t1 = torch.tensor(l_1)\n",
    "t2 = torch.tensor(l_2)\n",
    "t3 = torch.tensor(l_3)\n",
    "\n",
    "\"\"\"\n",
    "addition 加\n",
    "subtraction 减\n",
    "multiplication 乘\n",
    "division 除\n",
    "\"\"\"\n",
    "\n",
    "add = t1 + t2 + t3\n",
    "subtract = t3 - t2 -t1\n",
    "multiply = t1 * t2\n",
    "divide = t2 / t1\n",
    "print(\"下面开始张量元素的加减乘除运算：\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"Add: \\n{add}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"Subtract: \\n{subtract}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"Multiply: \\n{multiply}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"Divide: \\n{divide}\")\n",
    "\n",
    "print(\"+\" * 70)\n",
    "print(\"\\n\")\n",
    "l_ = [\n",
    "    [1, 2],\n",
    "    [1, 2]\n",
    "]\n",
    "\n",
    "_l = [\n",
    "    [1, 2],\n",
    "    [1, 2]\n",
    "]\n",
    "\n",
    "te1 = torch.tensor(l_)\n",
    "te2 = torch.tensor(_l)\n",
    "\n",
    "y1 = te1 @ te2\n",
    "y2 = te1.matmul(te2)\n",
    "y3 = torch.ones_like(y1)\n",
    "torch.matmul(te1, te2, out=y3)\n",
    "print(\"下面通过三种方式，进行张量的矩阵乘法运算：\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"这是第一种，通过‘@’进行运算： \\n{y1}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"这是第二种，通过matmul方法进行计算： \\n{y2}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"这是第三种，和种类似但是需要将积指向一个初始化的张量： \\n{y3}\")\n",
    "\n",
    "print(\"\\n\" *  4)\n",
    "# 张量的元素乘法也有类似的方法\n",
    "z1 = torch.tensor([\n",
    "    [1, 2, 3],\n",
    "    [1, 2, 3],\n",
    "    [1, 2, 3]\n",
    "])\n",
    "\n",
    "z2 = torch.tensor([\n",
    "    [4, 5, 6],\n",
    "    [4, 5, 6],\n",
    "    [4, 5, 6]\n",
    "])\n",
    "\n",
    "s1 = z1 * z2\n",
    "s2 = z1.mul(z2)\n",
    "s3 = torch.ones_like(s1)\n",
    "torch.mul(z1, z2, out=s3)\n",
    "print(\"下面是与张量的矩阵乘法相类似的元素乘法：\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"'*': \\n{s1}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"'.mul': \\n{s2}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"'torch.mul': \\n{s3}\")\n"
   ],
   "id": "6d51077ab1a12104",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "下面开始张量元素的加减乘除运算：\n",
      "----------------------------------------------------------------------\n",
      "Add: \n",
      "tensor([[5, 7, 9],\n",
      "        [5, 7, 9],\n",
      "        [5, 7, 9]])\n",
      "----------------------------------------------------------------------\n",
      "Subtract: \n",
      "tensor([[-5, -7, -9],\n",
      "        [-5, -7, -9],\n",
      "        [-5, -7, -9]])\n",
      "----------------------------------------------------------------------\n",
      "Multiply: \n",
      "tensor([[ 4, 10, 18],\n",
      "        [ 4, 10, 18],\n",
      "        [ 4, 10, 18]])\n",
      "----------------------------------------------------------------------\n",
      "Divide: \n",
      "tensor([[4.0000, 2.5000, 2.0000],\n",
      "        [4.0000, 2.5000, 2.0000],\n",
      "        [4.0000, 2.5000, 2.0000]])\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "下面通过三种方式，进行张量的矩阵乘法运算：\n",
      "----------------------------------------------------------------------\n",
      "这是第一种，通过‘@’进行运算： \n",
      "tensor([[3, 6],\n",
      "        [3, 6]])\n",
      "----------------------------------------------------------------------\n",
      "这是第二种，通过matmul方法进行计算： \n",
      "tensor([[3, 6],\n",
      "        [3, 6]])\n",
      "----------------------------------------------------------------------\n",
      "这是第三种，和种类似但是需要将积指向一个初始化的张量： \n",
      "tensor([[3, 6],\n",
      "        [3, 6]])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "下面是与张量的矩阵乘法相类似的元素乘法：\n",
      "----------------------------------------------------------------------\n",
      "'*': \n",
      "tensor([[ 4, 10, 18],\n",
      "        [ 4, 10, 18],\n",
      "        [ 4, 10, 18]])\n",
      "----------------------------------------------------------------------\n",
      "'.mul': \n",
      "tensor([[ 4, 10, 18],\n",
      "        [ 4, 10, 18],\n",
      "        [ 4, 10, 18]])\n",
      "----------------------------------------------------------------------\n",
      "'torch.mul': \n",
      "tensor([[ 4, 10, 18],\n",
      "        [ 4, 10, 18],\n",
      "        [ 4, 10, 18]])\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "张量可以进行算术运算和矩阵运算。进行算术运算时，可以将相同形状的张量直接进行加减乘除运算。对于矩阵乘法的运算，和算术乘法类似。\n",
    "方法一： 矩阵运算和算术运算分别使用 “@” 和 “*” 为运算符进行乘法运算。\n",
    "方法二： 分别使用 tensor1.matmul(tensor2) 和 tensor1.mul(tensor2) 进行运算。\n",
    "方法三： 在方法二的基础上对第三张量进行覆盖， 分别使用 torch.matmul(tensor1, tensor2, out=tensor3) 和 torch.mul(tensor1, tensor2, out=tensor3)进行运算。值得一提的是被覆盖的张量必须形状和元素类型必须和乘积的要完全相同。\n",
    "矩阵乘法： 就是用一个矩阵的一行和另一个矩阵的每一列元素进行乘法然后求和，乘积的形状依旧和原来的矩阵形状相同。"
   ],
   "id": "7232da2f7040031c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T12:01:57.768113Z",
     "start_time": "2025-06-01T12:01:57.736364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "t = torch.tensor([1, 2, 3])\n",
    "agg0 = t.sum()\n",
    "b = agg0.item()\n",
    "print(b)\n",
    "\n",
    "\n",
    "tensor = torch.tensor([\n",
    "    [0, 2],\n",
    "    [1, 2],\n",
    "])\n",
    "\n",
    "agg = tensor.sum()\n",
    "p = agg.item()\n",
    "print(f\"{p} is '{type(p)}'\")\n",
    "\n",
    "tensor2 = torch.tensor([\n",
    "    [\n",
    "        [1, 2, 3],\n",
    "        [1, 2, 3],\n",
    "        [1, 2, 3]\n",
    "    ],\n",
    "    [\n",
    "        [1, 2, 3],\n",
    "        [1, 2, 3],\n",
    "        [1, 2, 3]\n",
    "    ],\n",
    "    [\n",
    "        [1, 2, 3],\n",
    "        [1, 2, 3],\n",
    "        [1, 2, 3]\n",
    "    ]\n",
    "])\n",
    "\n",
    "agg2 = tensor2.sum()\n",
    "print(agg2)\n",
    "pn = agg2.item()\n",
    "print(f\"{pn} is {type(pn)}\")\n",
    "\n",
    "\n"
   ],
   "id": "a7c3a2c5d5a31dbe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "5 is '<class 'int'>'\n",
      "tensor(54)\n",
      "54 is <class 'int'>\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "虽然PyTorch官方说是单元素张量可以把他们聚合在一起形成一个值，然后可以把他变成Python数值使用。但是我发现好像不只单元素张量可以，多元素，高维的张量也可以。把他们聚合在一起，实际上就是把张量里面的数字全部加起来。可以通过内置函数sum()聚合，此时是tensor类型的数字。然后通过item()方法将其转换成python可利用的数字类型。",
   "id": "7c33460b76360321"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T12:39:52.977903Z",
     "start_time": "2025-06-01T12:39:52.936143Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\"\"\"\n",
    "原地操作\n",
    "\"\"\"\n",
    "\n",
    "tensor1 = torch.tensor([\n",
    "    [2, 1],\n",
    "    [2, 1]\n",
    "], dtype=torch.float64)\n",
    "\n",
    "tensor2 = torch.tensor([\n",
    "    [4, 2],\n",
    "    [4, 2]\n",
    "], dtype=torch.float64)\n",
    "\n",
    "# tensor1.add_(tensor2)\n",
    "# tensor2.sub_(tensor1)\n",
    "# tensor1.mul_(tensor2)\n",
    "# tensor1.div_(tensor2)\n",
    "# print(f\"加： \\n{tensor1}\")\n",
    "# print(f\"减： \\n{tensor2}\")\n",
    "# print(f\"乘： \\n{tensor1}\")\n",
    "# print(f\"除： \\n{tensor1}\")\n",
    "# tensor1.sub_(tensor2)\n",
    "# print(tensor1)\n",
    "# print(tensor2)\n",
    "\n",
    "# tensor1.copy_(tensor2)\n",
    "# v = tensor1.clone()\n",
    "# print(v)\n",
    "\n",
    "tensor1.add_(2)\n",
    "print(tensor1)\n",
    "tensor1.mul_(3)\n",
    "print(tensor1)\n",
    "tensor1.fill_(0)\n",
    "print(tensor1)\n",
    "\n",
    "\n"
   ],
   "id": "d7e939d12b01c879",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 3.],\n",
      "        [4., 3.]], dtype=torch.float64)\n",
      "tensor([[12.,  9.],\n",
      "        [12.,  9.]], dtype=torch.float64)\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "原地操作，也就是不建立一个新的变量来承接运算的值，而是直接在原有的张量变量上进行修改。就类似于上面的那个矩阵乘法运算的方法一。\n",
    "常见的原地操作函数有：\n",
    "a:add_()\n",
    "b:sub_()\n",
    "c:mul_()\n",
    "d:div_()\n",
    "e:fill_()\n",
    "f:copy_()\n",
    "也就是对应的加减乘除四则运算以及填充。填充就是把原来的张量元素全部替换成括号里的数字，类似于前面那几个覆盖函数。值得一提的是这个不仅能张量和张量进行操作，而且也可以张量和数字一起操作，就向加减扩大倍数一样。\n",
    "整体形式是 tensor1.add_(tensor2) 是前面的操作后面的变量，例如tensor1 / tensor2 而后面的变量不会改变。\n",
    "还需要注意的是，当张量运算涉及除法时torch会自动把数字变成浮点数为了保证精度，所以如果原来的数据类型不是浮点数的话就会报错，这里需要注意。\n",
    "还有一个 clone() 函数，可以把一个张量克隆给其他变量，这个不是原地操作，形如 demo = tensor.clone()."
   ],
   "id": "9b6734e1b86e29f0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:01:29.260567Z",
     "start_time": "2025-06-01T13:01:29.252460Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\"\"\"\n",
    "使用numpy桥接\n",
    "\"\"\"\n",
    "\n",
    "# 从tensor到numpy\n",
    "tensor = torch.ones(4, 4)\n",
    "array = tensor.numpy()\n",
    "print(tensor)\n",
    "print(array)\n",
    "print(\"-\" * 70)\n",
    "# 从numpy到tensor\n",
    "arr = np.ones((3, 3), dtype=np.float64)\n",
    "ten = torch.from_numpy(arr)\n",
    "print(arr)\n",
    "print(ten)\n"
   ],
   "id": "657c217d5241eb53",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]]\n",
      "----------------------------------------------------------------------\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "numpy数组可以和torch的张量进行相互转换，并且共享内存。转换前后数据类型，形状，设备信息都会保持一致。\n",
    "数组转换成张量： tensor = torch.from_numpy(array)\n",
    "张量转换成数组： array = tensor.numpy()"
   ],
   "id": "926171c95f83abcc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
