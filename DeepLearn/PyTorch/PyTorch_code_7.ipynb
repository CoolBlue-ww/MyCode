{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-06T09:10:37.919998Z",
     "start_time": "2025-06-06T09:06:05.678344Z"
    }
   },
   "source": [
    "# 优化参数\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\"\"\"加载数据集\"\"\"\n",
    "\n",
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "\"\"\"检测设备\"\"\"\n",
    "device = torch.accelerator.current_accelerator() if torch.accelerator.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "\"\"\"组建神经网络\"\"\"\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            # nn.Linear(128, 64),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(64, 32),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(32, 16),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(16, 10)\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.stack(x)\n",
    "        return logits\n",
    "\n",
    "\"\"\"将模型转移到设备上\"\"\"\n",
    "model = NeuralNetwork().to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('PyTorch_code_7_pth/model_2.pth', weights_only=True))\n",
    "\n",
    "\"\"\"生成迭代器DataLoader\"\"\"\n",
    "# 批次大小\n",
    "batch_size = 50\n",
    "\n",
    "train_DataLoader = DataLoader(dataset=train_data, batch_size=batch_size)\n",
    "test_DataLoader = DataLoader(dataset=test_data, batch_size=batch_size)\n",
    "\n",
    "\"\"\"定义损失函数和优化器\"\"\"\n",
    "loss_def = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "\n",
    "# 训练轮数\n",
    "epochs = 100\n",
    "\n",
    "# 记录每一轮验证集上的损失函数值和正确率。\n",
    "Epoch = [index for index in range(1, epochs + 1)]\n",
    "Test_Loss = []\n",
    "Correctness = []\n",
    "\n",
    "# 封装训练函数\n",
    "def train(model, dataloader, loss_def, optim):\n",
    "    data_size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    count = 0\n",
    "    for Batch, (Images, Labels) in enumerate(dataloader):\n",
    "        Images, Labels = Images.to(device), Labels.to(device)\n",
    "        logits = model(Images)\n",
    "        loss = loss_def(logits, Labels)\n",
    "        loss.backward()\n",
    "        count += 1\n",
    "        if count % 4 == 0:\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "        if (Batch + 1) % 100 == 0:\n",
    "            loss, current = loss.item(), (Batch + 1) * len(Images)\n",
    "            print(f\"Loss: {loss:>6f}   |   [current: {current:>5d}/{data_size}]\")\n",
    "    return\n",
    "\n",
    "\n",
    "# 封装验证函数\n",
    "def test(model, dataloader, loss_def):\n",
    "    data_size = len(dataloader.dataset)\n",
    "    batchs = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch, (Image, Label) in enumerate(dataloader):\n",
    "            Image, Label = Image.to(device), Label.to(device)\n",
    "            logits = model(Image)\n",
    "            test_loss += loss_def(logits, Label)\n",
    "            correct += (logits.argmax(1) == Label).type(torch.float).sum().item()\n",
    "    result = correct / data_size\n",
    "    batch_loss = test_loss / batchs\n",
    "    print(f\"Test Error:\\ntest_correct: {result:>.6f}   |   [test_loss: {batch_loss:>.6f}]\")\n",
    "    return result, batch_loss\n",
    "\n",
    "\n",
    "# 进行循环多轮训练\n",
    "for index in range(epochs):\n",
    "    print(f\"Epochs {index + 1}\" + \"\\n+\" + \"-\" * 80)\n",
    "    train(model, train_DataLoader, loss_def, optim)\n",
    "    result, batch_loss = test(model, test_DataLoader, loss_def)\n",
    "    Correctness.append(result)\n",
    "    Test_Loss.append(batch_size)\n",
    "\n",
    "# 保存模型训练参数\n",
    "path = \"PyTorch_code_7_pth/model_3.pth\"\n",
    "torch.save(model.state_dict(), path)\n",
    "print(f\"模型参数已保存到 '{path}'\")\n",
    "\n",
    "\n",
    "# 保存csv文件\n",
    "data = {\n",
    "    'Epoch': Epoch,\n",
    "    'Test_Loss': Test_Loss,\n",
    "    'Correctness': Correctness\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "out_path = \"/home/SayMyName/桌面/GitHub/MyCode/DeepLearn/PyTorch/PyTorch_code_7_CSV\"\n",
    "df.to_csv(out_path, index=False)\n",
    "print(f\"CSV文件已经保存到 {out_path}\")\n",
    "\n",
    "# 加载模型参数\n",
    "# model = NeuralNetwork().to(device)\n",
    "# model.load_state_dict(torch.load(\"PyTorch_code_7_pth/model_1.pth\", weights_only=True))\n",
    "#\n",
    "#\n",
    "# # 进行预测\n",
    "# classes = [\n",
    "#     \"T-shirt/top\",\n",
    "#     \"Trouser\",\n",
    "#     \"Pullover\",\n",
    "#     \"Dress\",\n",
    "#     \"Coat\",\n",
    "#     \"Sandal\",\n",
    "#     \"Shirt\",\n",
    "#     \"Sneaker\",\n",
    "#     \"Bag\",\n",
    "#     \"Ankle boot\",\n",
    "# ]\n",
    "#\n",
    "# rand = random.randint(0, 1000)\n",
    "#\n",
    "# model.eval()\n",
    "# Image, Label = test_data[rand][0], test_data[rand][1]\n",
    "# with torch.no_grad():\n",
    "#     Image = Image.to(device)\n",
    "#     predict = model(Image)\n",
    "#     print(predict)\n",
    "#     print(predict[0].argmax(0))\n",
    "#     predicted, actual = classes[predict[0].argmax(0)], classes[Label]\n",
    "#\n",
    "# print(f\"Predicted: {predicted}   |   Actual: {actual}\")\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs 1\n",
      "+--------------------------------------------------------------------------------\n",
      "Loss: 0.509129   |   [current:  5000/60000]\n",
      "Loss: 0.553334   |   [current: 10000/60000]\n",
      "Loss: 0.594932   |   [current: 15000/60000]\n",
      "Loss: 0.541656   |   [current: 20000/60000]\n",
      "Loss: 0.640943   |   [current: 25000/60000]\n",
      "Loss: 0.349551   |   [current: 30000/60000]\n",
      "Loss: 0.546250   |   [current: 35000/60000]\n",
      "Loss: 0.518914   |   [current: 40000/60000]\n",
      "Loss: 0.495469   |   [current: 45000/60000]\n",
      "Loss: 0.742944   |   [current: 50000/60000]\n",
      "Loss: 0.537954   |   [current: 55000/60000]\n",
      "Loss: 0.503794   |   [current: 60000/60000]\n",
      "Test Error:\n",
      "test_correct: 0.814500   |   [test_loss: 0.526984]\n",
      "Epochs 2\n",
      "+--------------------------------------------------------------------------------\n",
      "Loss: 0.508326   |   [current:  5000/60000]\n",
      "Loss: 0.552458   |   [current: 10000/60000]\n",
      "Loss: 0.594540   |   [current: 15000/60000]\n",
      "Loss: 0.541476   |   [current: 20000/60000]\n",
      "Loss: 0.640637   |   [current: 25000/60000]\n",
      "Loss: 0.349095   |   [current: 30000/60000]\n",
      "Loss: 0.545504   |   [current: 35000/60000]\n",
      "Loss: 0.518771   |   [current: 40000/60000]\n",
      "Loss: 0.494749   |   [current: 45000/60000]\n",
      "Loss: 0.742608   |   [current: 50000/60000]\n",
      "Loss: 0.537642   |   [current: 55000/60000]\n",
      "Loss: 0.503325   |   [current: 60000/60000]\n",
      "Test Error:\n",
      "test_correct: 0.814700   |   [test_loss: 0.526560]\n",
      "Epochs 3\n",
      "+--------------------------------------------------------------------------------\n",
      "Loss: 0.507523   |   [current:  5000/60000]\n",
      "Loss: 0.551584   |   [current: 10000/60000]\n",
      "Loss: 0.594152   |   [current: 15000/60000]\n",
      "Loss: 0.541307   |   [current: 20000/60000]\n",
      "Loss: 0.640330   |   [current: 25000/60000]\n",
      "Loss: 0.348645   |   [current: 30000/60000]\n",
      "Loss: 0.544771   |   [current: 35000/60000]\n",
      "Loss: 0.518634   |   [current: 40000/60000]\n",
      "Loss: 0.494037   |   [current: 45000/60000]\n",
      "Loss: 0.742280   |   [current: 50000/60000]\n",
      "Loss: 0.537331   |   [current: 55000/60000]\n",
      "Loss: 0.502855   |   [current: 60000/60000]\n",
      "Test Error:\n",
      "test_correct: 0.815000   |   [test_loss: 0.526139]\n",
      "Epochs 4\n",
      "+--------------------------------------------------------------------------------\n",
      "Loss: 0.506728   |   [current:  5000/60000]\n",
      "Loss: 0.550712   |   [current: 10000/60000]\n",
      "Loss: 0.593765   |   [current: 15000/60000]\n",
      "Loss: 0.541142   |   [current: 20000/60000]\n",
      "Loss: 0.640022   |   [current: 25000/60000]\n",
      "Loss: 0.348194   |   [current: 30000/60000]\n",
      "Loss: 0.544045   |   [current: 35000/60000]\n",
      "Loss: 0.518499   |   [current: 40000/60000]\n",
      "Loss: 0.493327   |   [current: 45000/60000]\n",
      "Loss: 0.741957   |   [current: 50000/60000]\n",
      "Loss: 0.537023   |   [current: 55000/60000]\n",
      "Loss: 0.502391   |   [current: 60000/60000]\n",
      "Test Error:\n",
      "test_correct: 0.814900   |   [test_loss: 0.525721]\n",
      "Epochs 5\n",
      "+--------------------------------------------------------------------------------\n",
      "Loss: 0.505934   |   [current:  5000/60000]\n",
      "Loss: 0.549847   |   [current: 10000/60000]\n",
      "Loss: 0.593380   |   [current: 15000/60000]\n",
      "Loss: 0.540982   |   [current: 20000/60000]\n",
      "Loss: 0.639717   |   [current: 25000/60000]\n",
      "Loss: 0.347755   |   [current: 30000/60000]\n",
      "Loss: 0.543322   |   [current: 35000/60000]\n",
      "Loss: 0.518363   |   [current: 40000/60000]\n",
      "Loss: 0.492618   |   [current: 45000/60000]\n",
      "Loss: 0.741634   |   [current: 50000/60000]\n",
      "Loss: 0.536720   |   [current: 55000/60000]\n",
      "Loss: 0.501932   |   [current: 60000/60000]\n",
      "Test Error:\n",
      "test_correct: 0.815200   |   [test_loss: 0.525305]\n",
      "Epochs 6\n",
      "+--------------------------------------------------------------------------------\n",
      "Loss: 0.505142   |   [current:  5000/60000]\n",
      "Loss: 0.548986   |   [current: 10000/60000]\n",
      "Loss: 0.592998   |   [current: 15000/60000]\n",
      "Loss: 0.540814   |   [current: 20000/60000]\n",
      "Loss: 0.639411   |   [current: 25000/60000]\n",
      "Loss: 0.347320   |   [current: 30000/60000]\n",
      "Loss: 0.542606   |   [current: 35000/60000]\n",
      "Loss: 0.518236   |   [current: 40000/60000]\n",
      "Loss: 0.491916   |   [current: 45000/60000]\n",
      "Loss: 0.741319   |   [current: 50000/60000]\n",
      "Loss: 0.536422   |   [current: 55000/60000]\n",
      "Loss: 0.501478   |   [current: 60000/60000]\n",
      "Test Error:\n",
      "test_correct: 0.815300   |   [test_loss: 0.524892]\n",
      "Epochs 7\n",
      "+--------------------------------------------------------------------------------\n",
      "Loss: 0.504355   |   [current:  5000/60000]\n",
      "Loss: 0.548128   |   [current: 10000/60000]\n",
      "Loss: 0.592618   |   [current: 15000/60000]\n",
      "Loss: 0.540654   |   [current: 20000/60000]\n",
      "Loss: 0.639108   |   [current: 25000/60000]\n",
      "Loss: 0.346893   |   [current: 30000/60000]\n",
      "Loss: 0.541890   |   [current: 35000/60000]\n",
      "Loss: 0.518106   |   [current: 40000/60000]\n",
      "Loss: 0.491211   |   [current: 45000/60000]\n",
      "Loss: 0.741005   |   [current: 50000/60000]\n",
      "Loss: 0.536133   |   [current: 55000/60000]\n",
      "Loss: 0.501032   |   [current: 60000/60000]\n",
      "Test Error:\n",
      "test_correct: 0.815500   |   [test_loss: 0.524482]\n",
      "Epochs 8\n",
      "+--------------------------------------------------------------------------------\n",
      "Loss: 0.503565   |   [current:  5000/60000]\n",
      "Loss: 0.547276   |   [current: 10000/60000]\n",
      "Loss: 0.592240   |   [current: 15000/60000]\n",
      "Loss: 0.540488   |   [current: 20000/60000]\n",
      "Loss: 0.638806   |   [current: 25000/60000]\n",
      "Loss: 0.346467   |   [current: 30000/60000]\n",
      "Loss: 0.541179   |   [current: 35000/60000]\n",
      "Loss: 0.517977   |   [current: 40000/60000]\n",
      "Loss: 0.490517   |   [current: 45000/60000]\n",
      "Loss: 0.740696   |   [current: 50000/60000]\n",
      "Loss: 0.535838   |   [current: 55000/60000]\n",
      "Loss: 0.500588   |   [current: 60000/60000]\n",
      "Test Error:\n",
      "test_correct: 0.815600   |   [test_loss: 0.524074]\n",
      "Epochs 9\n",
      "+--------------------------------------------------------------------------------\n",
      "Loss: 0.502778   |   [current:  5000/60000]\n",
      "Loss: 0.546430   |   [current: 10000/60000]\n",
      "Loss: 0.591864   |   [current: 15000/60000]\n",
      "Loss: 0.540330   |   [current: 20000/60000]\n",
      "Loss: 0.638507   |   [current: 25000/60000]\n",
      "Loss: 0.346047   |   [current: 30000/60000]\n",
      "Loss: 0.540481   |   [current: 35000/60000]\n",
      "Loss: 0.517849   |   [current: 40000/60000]\n",
      "Loss: 0.489830   |   [current: 45000/60000]\n",
      "Loss: 0.740391   |   [current: 50000/60000]\n",
      "Loss: 0.535547   |   [current: 55000/60000]\n",
      "Loss: 0.500149   |   [current: 60000/60000]\n",
      "Test Error:\n",
      "test_correct: 0.815800   |   [test_loss: 0.523669]\n",
      "Epochs 10\n",
      "+--------------------------------------------------------------------------------\n",
      "Loss: 0.501993   |   [current:  5000/60000]\n",
      "Loss: 0.545586   |   [current: 10000/60000]\n",
      "Loss: 0.591489   |   [current: 15000/60000]\n",
      "Loss: 0.540169   |   [current: 20000/60000]\n",
      "Loss: 0.638210   |   [current: 25000/60000]\n",
      "Loss: 0.345635   |   [current: 30000/60000]\n",
      "Loss: 0.539790   |   [current: 35000/60000]\n",
      "Loss: 0.517720   |   [current: 40000/60000]\n",
      "Loss: 0.489147   |   [current: 45000/60000]\n",
      "Loss: 0.740086   |   [current: 50000/60000]\n",
      "Loss: 0.535261   |   [current: 55000/60000]\n",
      "Loss: 0.499713   |   [current: 60000/60000]\n",
      "Test Error:\n",
      "test_correct: 0.815900   |   [test_loss: 0.523267]\n",
      "Epochs 11\n",
      "+--------------------------------------------------------------------------------\n",
      "Loss: 0.501210   |   [current:  5000/60000]\n",
      "Loss: 0.544747   |   [current: 10000/60000]\n",
      "Loss: 0.591115   |   [current: 15000/60000]\n",
      "Loss: 0.540014   |   [current: 20000/60000]\n",
      "Loss: 0.637908   |   [current: 25000/60000]\n",
      "Loss: 0.345226   |   [current: 30000/60000]\n",
      "Loss: 0.539106   |   [current: 35000/60000]\n",
      "Loss: 0.517594   |   [current: 40000/60000]\n",
      "Loss: 0.488466   |   [current: 45000/60000]\n",
      "Loss: 0.739791   |   [current: 50000/60000]\n",
      "Loss: 0.534980   |   [current: 55000/60000]\n",
      "Loss: 0.499283   |   [current: 60000/60000]\n",
      "Test Error:\n",
      "test_correct: 0.816200   |   [test_loss: 0.522868]\n",
      "Epochs 12\n",
      "+--------------------------------------------------------------------------------\n",
      "Loss: 0.500428   |   [current:  5000/60000]\n",
      "Loss: 0.543911   |   [current: 10000/60000]\n",
      "Loss: 0.590742   |   [current: 15000/60000]\n",
      "Loss: 0.539857   |   [current: 20000/60000]\n",
      "Loss: 0.637606   |   [current: 25000/60000]\n",
      "Loss: 0.344820   |   [current: 30000/60000]\n",
      "Loss: 0.538422   |   [current: 35000/60000]\n",
      "Loss: 0.517467   |   [current: 40000/60000]\n",
      "Loss: 0.487790   |   [current: 45000/60000]\n",
      "Loss: 0.739503   |   [current: 50000/60000]\n",
      "Loss: 0.534700   |   [current: 55000/60000]\n",
      "Loss: 0.498855   |   [current: 60000/60000]\n",
      "Test Error:\n",
      "test_correct: 0.816700   |   [test_loss: 0.522471]\n",
      "Epochs 13\n",
      "+--------------------------------------------------------------------------------\n",
      "Loss: 0.499652   |   [current:  5000/60000]\n",
      "Loss: 0.543076   |   [current: 10000/60000]\n",
      "Loss: 0.590371   |   [current: 15000/60000]\n",
      "Loss: 0.539702   |   [current: 20000/60000]\n",
      "Loss: 0.637313   |   [current: 25000/60000]\n",
      "Loss: 0.344418   |   [current: 30000/60000]\n",
      "Loss: 0.537747   |   [current: 35000/60000]\n",
      "Loss: 0.517340   |   [current: 40000/60000]\n",
      "Loss: 0.487117   |   [current: 45000/60000]\n",
      "Loss: 0.739213   |   [current: 50000/60000]\n",
      "Loss: 0.534423   |   [current: 55000/60000]\n",
      "Loss: 0.498429   |   [current: 60000/60000]\n",
      "Test Error:\n",
      "test_correct: 0.816700   |   [test_loss: 0.522077]\n",
      "Epochs 14\n",
      "+--------------------------------------------------------------------------------\n",
      "Loss: 0.498883   |   [current:  5000/60000]\n",
      "Loss: 0.542241   |   [current: 10000/60000]\n",
      "Loss: 0.589999   |   [current: 15000/60000]\n",
      "Loss: 0.539552   |   [current: 20000/60000]\n",
      "Loss: 0.637024   |   [current: 25000/60000]\n",
      "Loss: 0.344021   |   [current: 30000/60000]\n",
      "Loss: 0.537077   |   [current: 35000/60000]\n",
      "Loss: 0.517205   |   [current: 40000/60000]\n",
      "Loss: 0.486445   |   [current: 45000/60000]\n",
      "Loss: 0.738929   |   [current: 50000/60000]\n",
      "Loss: 0.534149   |   [current: 55000/60000]\n",
      "Loss: 0.498007   |   [current: 60000/60000]\n",
      "Test Error:\n",
      "test_correct: 0.816700   |   [test_loss: 0.521685]\n",
      "Epochs 15\n",
      "+--------------------------------------------------------------------------------\n",
      "Loss: 0.498122   |   [current:  5000/60000]\n",
      "Loss: 0.541413   |   [current: 10000/60000]\n",
      "Loss: 0.589628   |   [current: 15000/60000]\n",
      "Loss: 0.539401   |   [current: 20000/60000]\n",
      "Loss: 0.636728   |   [current: 25000/60000]\n",
      "Loss: 0.343630   |   [current: 30000/60000]\n",
      "Loss: 0.536404   |   [current: 35000/60000]\n",
      "Loss: 0.517080   |   [current: 40000/60000]\n",
      "Loss: 0.485783   |   [current: 45000/60000]\n",
      "Loss: 0.738647   |   [current: 50000/60000]\n",
      "Loss: 0.533872   |   [current: 55000/60000]\n",
      "Loss: 0.497586   |   [current: 60000/60000]\n",
      "Test Error:\n",
      "test_correct: 0.816900   |   [test_loss: 0.521296]\n",
      "Epochs 16\n",
      "+--------------------------------------------------------------------------------\n",
      "Loss: 0.497360   |   [current:  5000/60000]\n",
      "Loss: 0.540588   |   [current: 10000/60000]\n",
      "Loss: 0.589259   |   [current: 15000/60000]\n",
      "Loss: 0.539250   |   [current: 20000/60000]\n",
      "Loss: 0.636431   |   [current: 25000/60000]\n",
      "Loss: 0.343244   |   [current: 30000/60000]\n",
      "Loss: 0.535739   |   [current: 35000/60000]\n",
      "Loss: 0.516952   |   [current: 40000/60000]\n",
      "Loss: 0.485123   |   [current: 45000/60000]\n",
      "Loss: 0.738365   |   [current: 50000/60000]\n",
      "Loss: 0.533601   |   [current: 55000/60000]\n",
      "Loss: 0.497175   |   [current: 60000/60000]\n",
      "Test Error:\n",
      "test_correct: 0.816800   |   [test_loss: 0.520910]\n",
      "Epochs 17\n",
      "+--------------------------------------------------------------------------------\n",
      "Loss: 0.496600   |   [current:  5000/60000]\n",
      "Loss: 0.539760   |   [current: 10000/60000]\n",
      "Loss: 0.588895   |   [current: 15000/60000]\n",
      "Loss: 0.539103   |   [current: 20000/60000]\n",
      "Loss: 0.636126   |   [current: 25000/60000]\n",
      "Loss: 0.342869   |   [current: 30000/60000]\n",
      "Loss: 0.535076   |   [current: 35000/60000]\n",
      "Loss: 0.516830   |   [current: 40000/60000]\n",
      "Loss: 0.484469   |   [current: 45000/60000]\n",
      "Loss: 0.738086   |   [current: 50000/60000]\n",
      "Loss: 0.533340   |   [current: 55000/60000]\n",
      "Loss: 0.496763   |   [current: 60000/60000]\n",
      "Test Error:\n",
      "test_correct: 0.817000   |   [test_loss: 0.520526]\n",
      "Epochs 18\n",
      "+--------------------------------------------------------------------------------\n",
      "Loss: 0.495843   |   [current:  5000/60000]\n",
      "Loss: 0.538934   |   [current: 10000/60000]\n",
      "Loss: 0.588533   |   [current: 15000/60000]\n",
      "Loss: 0.538954   |   [current: 20000/60000]\n",
      "Loss: 0.635821   |   [current: 25000/60000]\n",
      "Loss: 0.342502   |   [current: 30000/60000]\n",
      "Loss: 0.534417   |   [current: 35000/60000]\n",
      "Loss: 0.516710   |   [current: 40000/60000]\n",
      "Loss: 0.483819   |   [current: 45000/60000]\n",
      "Loss: 0.737811   |   [current: 50000/60000]\n",
      "Loss: 0.533084   |   [current: 55000/60000]\n",
      "Loss: 0.496356   |   [current: 60000/60000]\n",
      "Test Error:\n",
      "test_correct: 0.817000   |   [test_loss: 0.520144]\n",
      "Epochs 19\n",
      "+--------------------------------------------------------------------------------\n",
      "Loss: 0.495086   |   [current:  5000/60000]\n",
      "Loss: 0.538118   |   [current: 10000/60000]\n",
      "Loss: 0.588173   |   [current: 15000/60000]\n",
      "Loss: 0.538808   |   [current: 20000/60000]\n",
      "Loss: 0.635510   |   [current: 25000/60000]\n",
      "Loss: 0.342140   |   [current: 30000/60000]\n",
      "Loss: 0.533765   |   [current: 35000/60000]\n",
      "Loss: 0.516589   |   [current: 40000/60000]\n",
      "Loss: 0.483175   |   [current: 45000/60000]\n",
      "Loss: 0.737537   |   [current: 50000/60000]\n",
      "Loss: 0.532834   |   [current: 55000/60000]\n",
      "Loss: 0.495949   |   [current: 60000/60000]\n",
      "Test Error:\n",
      "test_correct: 0.817300   |   [test_loss: 0.519765]\n",
      "Epochs 20\n",
      "+--------------------------------------------------------------------------------\n",
      "Loss: 0.494331   |   [current:  5000/60000]\n",
      "Loss: 0.537302   |   [current: 10000/60000]\n",
      "Loss: 0.587817   |   [current: 15000/60000]\n",
      "Loss: 0.538670   |   [current: 20000/60000]\n",
      "Loss: 0.635207   |   [current: 25000/60000]\n",
      "Loss: 0.341784   |   [current: 30000/60000]\n",
      "Loss: 0.533124   |   [current: 35000/60000]\n",
      "Loss: 0.516471   |   [current: 40000/60000]\n",
      "Loss: 0.482539   |   [current: 45000/60000]\n",
      "Loss: 0.737268   |   [current: 50000/60000]\n",
      "Loss: 0.532587   |   [current: 55000/60000]\n",
      "Loss: 0.495554   |   [current: 60000/60000]\n",
      "Test Error:\n",
      "test_correct: 0.817500   |   [test_loss: 0.519388]\n",
      "Epochs 21\n",
      "+--------------------------------------------------------------------------------\n",
      "Loss: 0.493585   |   [current:  5000/60000]\n",
      "Loss: 0.536484   |   [current: 10000/60000]\n",
      "Loss: 0.587460   |   [current: 15000/60000]\n",
      "Loss: 0.538537   |   [current: 20000/60000]\n",
      "Loss: 0.634906   |   [current: 25000/60000]\n",
      "Loss: 0.341434   |   [current: 30000/60000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 126\u001B[39m\n\u001B[32m    124\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m index \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[32m    125\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mEpochs \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mindex\u001B[38;5;250m \u001B[39m+\u001B[38;5;250m \u001B[39m\u001B[32m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m + \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m+\u001B[39m\u001B[33m\"\u001B[39m + \u001B[33m\"\u001B[39m\u001B[33m-\u001B[39m\u001B[33m\"\u001B[39m * \u001B[32m80\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m126\u001B[39m     \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_DataLoader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_def\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptim\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    127\u001B[39m     result, batch_loss = test(model, test_DataLoader, loss_def)\n\u001B[32m    128\u001B[39m     Correctness.append(result)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 93\u001B[39m, in \u001B[36mtrain\u001B[39m\u001B[34m(model, dataloader, loss_def, optim)\u001B[39m\n\u001B[32m     91\u001B[39m Images, Labels = Images.to(device), Labels.to(device)\n\u001B[32m     92\u001B[39m logits = model(Images)\n\u001B[32m---> \u001B[39m\u001B[32m93\u001B[39m loss = \u001B[43mloss_def\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlogits\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mLabels\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     94\u001B[39m loss.backward()\n\u001B[32m     95\u001B[39m count += \u001B[32m1\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/Hello-World/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/Hello-World/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/Hello-World/lib/python3.12/site-packages/torch/nn/modules/loss.py:1297\u001B[39m, in \u001B[36mCrossEntropyLoss.forward\u001B[39m\u001B[34m(self, input, target)\u001B[39m\n\u001B[32m   1296\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, target: Tensor) -> Tensor:\n\u001B[32m-> \u001B[39m\u001B[32m1297\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcross_entropy\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1298\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   1299\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1300\u001B[39m \u001B[43m        \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1301\u001B[39m \u001B[43m        \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1302\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreduction\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mreduction\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1303\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlabel_smoothing\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mlabel_smoothing\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1304\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/Hello-World/lib/python3.12/site-packages/torch/nn/functional.py:3494\u001B[39m, in \u001B[36mcross_entropy\u001B[39m\u001B[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001B[39m\n\u001B[32m   3492\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   3493\u001B[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001B[32m-> \u001B[39m\u001B[32m3494\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_C\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_nn\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcross_entropy_loss\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   3495\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   3496\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3497\u001B[39m \u001B[43m    \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3498\u001B[39m \u001B[43m    \u001B[49m\u001B[43m_Reduction\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_enum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreduction\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3499\u001B[39m \u001B[43m    \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3500\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlabel_smoothing\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3501\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
