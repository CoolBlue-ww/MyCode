{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-27T13:34:43.381098Z",
     "start_time": "2025-05-27T13:16:12.147966Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break\n",
    "\n",
    "\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:5d}]\")\n",
    "\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100 * correct):>0.1f}%, Avg loss: {test_loss:>8f}\\n\")\n",
    "\n",
    "\n",
    "epochs = 500\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t + 1}\\n------------------------------------------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")\n",
    "\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))\n",
    "\n",
    "\n",
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    x = x.to(device)\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f\"Predicted: '{predicted}', Actual: '{actual}'\")\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 1\n",
      "------------------------------------------------------------------\n",
      "loss: 2.300407 [   64/10000]\n",
      "loss: 2.289511 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 23.8%, Avg loss: 2.277176\n",
      "\n",
      "Epoch 2\n",
      "------------------------------------------------------------------\n",
      "loss: 2.277748 [   64/10000]\n",
      "loss: 2.265134 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 35.3%, Avg loss: 2.254876\n",
      "\n",
      "Epoch 3\n",
      "------------------------------------------------------------------\n",
      "loss: 2.255738 [   64/10000]\n",
      "loss: 2.240659 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 44.1%, Avg loss: 2.231892\n",
      "\n",
      "Epoch 4\n",
      "------------------------------------------------------------------\n",
      "loss: 2.232854 [   64/10000]\n",
      "loss: 2.214851 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 49.8%, Avg loss: 2.207242\n",
      "\n",
      "Epoch 5\n",
      "------------------------------------------------------------------\n",
      "loss: 2.208336 [   64/10000]\n",
      "loss: 2.187196 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 51.0%, Avg loss: 2.180290\n",
      "\n",
      "Epoch 6\n",
      "------------------------------------------------------------------\n",
      "loss: 2.181380 [   64/10000]\n",
      "loss: 2.157198 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 51.3%, Avg loss: 2.150444\n",
      "\n",
      "Epoch 7\n",
      "------------------------------------------------------------------\n",
      "loss: 2.151640 [   64/10000]\n",
      "loss: 2.124261 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 52.1%, Avg loss: 2.117165\n",
      "\n",
      "Epoch 8\n",
      "------------------------------------------------------------------\n",
      "loss: 2.118526 [   64/10000]\n",
      "loss: 2.088112 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 2.079962\n",
      "\n",
      "Epoch 9\n",
      "------------------------------------------------------------------\n",
      "loss: 2.081574 [   64/10000]\n",
      "loss: 2.048420 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 54.3%, Avg loss: 2.038354\n",
      "\n",
      "Epoch 10\n",
      "------------------------------------------------------------------\n",
      "loss: 2.040369 [   64/10000]\n",
      "loss: 2.004637 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 55.3%, Avg loss: 1.991835\n",
      "\n",
      "Epoch 11\n",
      "------------------------------------------------------------------\n",
      "loss: 1.994289 [   64/10000]\n",
      "loss: 1.956524 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 56.4%, Avg loss: 1.940226\n",
      "\n",
      "Epoch 12\n",
      "------------------------------------------------------------------\n",
      "loss: 1.943110 [   64/10000]\n",
      "loss: 1.904294 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 57.0%, Avg loss: 1.883746\n",
      "\n",
      "Epoch 13\n",
      "------------------------------------------------------------------\n",
      "loss: 1.886785 [   64/10000]\n",
      "loss: 1.848325 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 57.6%, Avg loss: 1.823227\n",
      "\n",
      "Epoch 14\n",
      "------------------------------------------------------------------\n",
      "loss: 1.826007 [   64/10000]\n",
      "loss: 1.789613 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 57.9%, Avg loss: 1.760025\n",
      "\n",
      "Epoch 15\n",
      "------------------------------------------------------------------\n",
      "loss: 1.761808 [   64/10000]\n",
      "loss: 1.729583 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 58.6%, Avg loss: 1.695854\n",
      "\n",
      "Epoch 16\n",
      "------------------------------------------------------------------\n",
      "loss: 1.696076 [   64/10000]\n",
      "loss: 1.669679 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 1.632464\n",
      "\n",
      "Epoch 17\n",
      "------------------------------------------------------------------\n",
      "loss: 1.630451 [   64/10000]\n",
      "loss: 1.611519 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 60.0%, Avg loss: 1.571356\n",
      "\n",
      "Epoch 18\n",
      "------------------------------------------------------------------\n",
      "loss: 1.566501 [   64/10000]\n",
      "loss: 1.556158 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 60.6%, Avg loss: 1.513545\n",
      "\n",
      "Epoch 19\n",
      "------------------------------------------------------------------\n",
      "loss: 1.505569 [   64/10000]\n",
      "loss: 1.504262 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 61.1%, Avg loss: 1.459602\n",
      "\n",
      "Epoch 20\n",
      "------------------------------------------------------------------\n",
      "loss: 1.448565 [   64/10000]\n",
      "loss: 1.456257 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Avg loss: 1.409721\n",
      "\n",
      "Epoch 21\n",
      "------------------------------------------------------------------\n",
      "loss: 1.395874 [   64/10000]\n",
      "loss: 1.412202 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 62.0%, Avg loss: 1.363809\n",
      "\n",
      "Epoch 22\n",
      "------------------------------------------------------------------\n",
      "loss: 1.347565 [   64/10000]\n",
      "loss: 1.371849 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 1.321641\n",
      "\n",
      "Epoch 23\n",
      "------------------------------------------------------------------\n",
      "loss: 1.303547 [   64/10000]\n",
      "loss: 1.334961 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 62.7%, Avg loss: 1.282943\n",
      "\n",
      "Epoch 24\n",
      "------------------------------------------------------------------\n",
      "loss: 1.263500 [   64/10000]\n",
      "loss: 1.301219 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Avg loss: 1.247439\n",
      "\n",
      "Epoch 25\n",
      "------------------------------------------------------------------\n",
      "loss: 1.226986 [   64/10000]\n",
      "loss: 1.270237 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Avg loss: 1.214839\n",
      "\n",
      "Epoch 26\n",
      "------------------------------------------------------------------\n",
      "loss: 1.193646 [   64/10000]\n",
      "loss: 1.241704 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Avg loss: 1.184870\n",
      "\n",
      "Epoch 27\n",
      "------------------------------------------------------------------\n",
      "loss: 1.163159 [   64/10000]\n",
      "loss: 1.215259 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Avg loss: 1.157269\n",
      "\n",
      "Epoch 28\n",
      "------------------------------------------------------------------\n",
      "loss: 1.135247 [   64/10000]\n",
      "loss: 1.190681 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 1.131806\n",
      "\n",
      "Epoch 29\n",
      "------------------------------------------------------------------\n",
      "loss: 1.109672 [   64/10000]\n",
      "loss: 1.167783 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 1.108257\n",
      "\n",
      "Epoch 30\n",
      "------------------------------------------------------------------\n",
      "loss: 1.086222 [   64/10000]\n",
      "loss: 1.146382 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 1.086429\n",
      "\n",
      "Epoch 31\n",
      "------------------------------------------------------------------\n",
      "loss: 1.064650 [   64/10000]\n",
      "loss: 1.126316 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 1.066160\n",
      "\n",
      "Epoch 32\n",
      "------------------------------------------------------------------\n",
      "loss: 1.044774 [   64/10000]\n",
      "loss: 1.107454 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 65.0%, Avg loss: 1.047300\n",
      "\n",
      "Epoch 33\n",
      "------------------------------------------------------------------\n",
      "loss: 1.026442 [   64/10000]\n",
      "loss: 1.089688 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 1.029712\n",
      "\n",
      "Epoch 34\n",
      "------------------------------------------------------------------\n",
      "loss: 1.009528 [   64/10000]\n",
      "loss: 1.072915 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 1.013283\n",
      "\n",
      "Epoch 35\n",
      "------------------------------------------------------------------\n",
      "loss: 0.993876 [   64/10000]\n",
      "loss: 1.057063 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.997903\n",
      "\n",
      "Epoch 36\n",
      "------------------------------------------------------------------\n",
      "loss: 0.979383 [   64/10000]\n",
      "loss: 1.042062 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.983477\n",
      "\n",
      "Epoch 37\n",
      "------------------------------------------------------------------\n",
      "loss: 0.965934 [   64/10000]\n",
      "loss: 1.027827 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.969915\n",
      "\n",
      "Epoch 38\n",
      "------------------------------------------------------------------\n",
      "loss: 0.953444 [   64/10000]\n",
      "loss: 1.014307 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 0.957142\n",
      "\n",
      "Epoch 39\n",
      "------------------------------------------------------------------\n",
      "loss: 0.941831 [   64/10000]\n",
      "loss: 1.001439 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.945085\n",
      "\n",
      "Epoch 40\n",
      "------------------------------------------------------------------\n",
      "loss: 0.931028 [   64/10000]\n",
      "loss: 0.989186 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.933685\n",
      "\n",
      "Epoch 41\n",
      "------------------------------------------------------------------\n",
      "loss: 0.920945 [   64/10000]\n",
      "loss: 0.977478 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.922888\n",
      "\n",
      "Epoch 42\n",
      "------------------------------------------------------------------\n",
      "loss: 0.911539 [   64/10000]\n",
      "loss: 0.966313 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.912646\n",
      "\n",
      "Epoch 43\n",
      "------------------------------------------------------------------\n",
      "loss: 0.902763 [   64/10000]\n",
      "loss: 0.955633 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.902914\n",
      "\n",
      "Epoch 44\n",
      "------------------------------------------------------------------\n",
      "loss: 0.894545 [   64/10000]\n",
      "loss: 0.945423 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.893652\n",
      "\n",
      "Epoch 45\n",
      "------------------------------------------------------------------\n",
      "loss: 0.886835 [   64/10000]\n",
      "loss: 0.935628 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 0.884822\n",
      "\n",
      "Epoch 46\n",
      "------------------------------------------------------------------\n",
      "loss: 0.879603 [   64/10000]\n",
      "loss: 0.926256 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.876397\n",
      "\n",
      "Epoch 47\n",
      "------------------------------------------------------------------\n",
      "loss: 0.872816 [   64/10000]\n",
      "loss: 0.917267 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 0.868342\n",
      "\n",
      "Epoch 48\n",
      "------------------------------------------------------------------\n",
      "loss: 0.866445 [   64/10000]\n",
      "loss: 0.908651 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 68.5%, Avg loss: 0.860633\n",
      "\n",
      "Epoch 49\n",
      "------------------------------------------------------------------\n",
      "loss: 0.860430 [   64/10000]\n",
      "loss: 0.900390 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 68.6%, Avg loss: 0.853248\n",
      "\n",
      "Epoch 50\n",
      "------------------------------------------------------------------\n",
      "loss: 0.854765 [   64/10000]\n",
      "loss: 0.892455 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.846163\n",
      "\n",
      "Epoch 51\n",
      "------------------------------------------------------------------\n",
      "loss: 0.849414 [   64/10000]\n",
      "loss: 0.884843 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 0.839361\n",
      "\n",
      "Epoch 52\n",
      "------------------------------------------------------------------\n",
      "loss: 0.844366 [   64/10000]\n",
      "loss: 0.877538 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 0.832822\n",
      "\n",
      "Epoch 53\n",
      "------------------------------------------------------------------\n",
      "loss: 0.839583 [   64/10000]\n",
      "loss: 0.870500 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 0.826528\n",
      "\n",
      "Epoch 54\n",
      "------------------------------------------------------------------\n",
      "loss: 0.835038 [   64/10000]\n",
      "loss: 0.863725 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.820463\n",
      "\n",
      "Epoch 55\n",
      "------------------------------------------------------------------\n",
      "loss: 0.830717 [   64/10000]\n",
      "loss: 0.857202 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 0.814613\n",
      "\n",
      "Epoch 56\n",
      "------------------------------------------------------------------\n",
      "loss: 0.826621 [   64/10000]\n",
      "loss: 0.850920 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 0.808964\n",
      "\n",
      "Epoch 57\n",
      "------------------------------------------------------------------\n",
      "loss: 0.822719 [   64/10000]\n",
      "loss: 0.844875 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 0.803502\n",
      "\n",
      "Epoch 58\n",
      "------------------------------------------------------------------\n",
      "loss: 0.819003 [   64/10000]\n",
      "loss: 0.839060 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 70.4%, Avg loss: 0.798216\n",
      "\n",
      "Epoch 59\n",
      "------------------------------------------------------------------\n",
      "loss: 0.815442 [   64/10000]\n",
      "loss: 0.833456 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.793094\n",
      "\n",
      "Epoch 60\n",
      "------------------------------------------------------------------\n",
      "loss: 0.812046 [   64/10000]\n",
      "loss: 0.828037 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.788125\n",
      "\n",
      "Epoch 61\n",
      "------------------------------------------------------------------\n",
      "loss: 0.808785 [   64/10000]\n",
      "loss: 0.822799 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.783302\n",
      "\n",
      "Epoch 62\n",
      "------------------------------------------------------------------\n",
      "loss: 0.805676 [   64/10000]\n",
      "loss: 0.817745 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 71.2%, Avg loss: 0.778616\n",
      "\n",
      "Epoch 63\n",
      "------------------------------------------------------------------\n",
      "loss: 0.802690 [   64/10000]\n",
      "loss: 0.812860 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.774061\n",
      "\n",
      "Epoch 64\n",
      "------------------------------------------------------------------\n",
      "loss: 0.799817 [   64/10000]\n",
      "loss: 0.808128 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 71.5%, Avg loss: 0.769627\n",
      "\n",
      "Epoch 65\n",
      "------------------------------------------------------------------\n",
      "loss: 0.797045 [   64/10000]\n",
      "loss: 0.803548 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 0.765306\n",
      "\n",
      "Epoch 66\n",
      "------------------------------------------------------------------\n",
      "loss: 0.794349 [   64/10000]\n",
      "loss: 0.799124 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 71.9%, Avg loss: 0.761093\n",
      "\n",
      "Epoch 67\n",
      "------------------------------------------------------------------\n",
      "loss: 0.791733 [   64/10000]\n",
      "loss: 0.794834 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 0.756982\n",
      "\n",
      "Epoch 68\n",
      "------------------------------------------------------------------\n",
      "loss: 0.789191 [   64/10000]\n",
      "loss: 0.790668 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 72.3%, Avg loss: 0.752965\n",
      "\n",
      "Epoch 69\n",
      "------------------------------------------------------------------\n",
      "loss: 0.786708 [   64/10000]\n",
      "loss: 0.786610 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 72.5%, Avg loss: 0.749040\n",
      "\n",
      "Epoch 70\n",
      "------------------------------------------------------------------\n",
      "loss: 0.784295 [   64/10000]\n",
      "loss: 0.782661 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 72.7%, Avg loss: 0.745199\n",
      "\n",
      "Epoch 71\n",
      "------------------------------------------------------------------\n",
      "loss: 0.781933 [   64/10000]\n",
      "loss: 0.778821 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 72.9%, Avg loss: 0.741438\n",
      "\n",
      "Epoch 72\n",
      "------------------------------------------------------------------\n",
      "loss: 0.779617 [   64/10000]\n",
      "loss: 0.775081 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.737753\n",
      "\n",
      "Epoch 73\n",
      "------------------------------------------------------------------\n",
      "loss: 0.777359 [   64/10000]\n",
      "loss: 0.771430 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 0.734138\n",
      "\n",
      "Epoch 74\n",
      "------------------------------------------------------------------\n",
      "loss: 0.775134 [   64/10000]\n",
      "loss: 0.767876 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 0.730591\n",
      "\n",
      "Epoch 75\n",
      "------------------------------------------------------------------\n",
      "loss: 0.772948 [   64/10000]\n",
      "loss: 0.764424 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 73.6%, Avg loss: 0.727109\n",
      "\n",
      "Epoch 76\n",
      "------------------------------------------------------------------\n",
      "loss: 0.770796 [   64/10000]\n",
      "loss: 0.761046 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 0.723687\n",
      "\n",
      "Epoch 77\n",
      "------------------------------------------------------------------\n",
      "loss: 0.768676 [   64/10000]\n",
      "loss: 0.757754 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 0.720324\n",
      "\n",
      "Epoch 78\n",
      "------------------------------------------------------------------\n",
      "loss: 0.766589 [   64/10000]\n",
      "loss: 0.754538 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 0.717018\n",
      "\n",
      "Epoch 79\n",
      "------------------------------------------------------------------\n",
      "loss: 0.764535 [   64/10000]\n",
      "loss: 0.751394 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.713765\n",
      "\n",
      "Epoch 80\n",
      "------------------------------------------------------------------\n",
      "loss: 0.762495 [   64/10000]\n",
      "loss: 0.748313 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 0.710565\n",
      "\n",
      "Epoch 81\n",
      "------------------------------------------------------------------\n",
      "loss: 0.760481 [   64/10000]\n",
      "loss: 0.745296 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.707415\n",
      "\n",
      "Epoch 82\n",
      "------------------------------------------------------------------\n",
      "loss: 0.758471 [   64/10000]\n",
      "loss: 0.742334 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.704315\n",
      "\n",
      "Epoch 83\n",
      "------------------------------------------------------------------\n",
      "loss: 0.756490 [   64/10000]\n",
      "loss: 0.739441 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 0.701261\n",
      "\n",
      "Epoch 84\n",
      "------------------------------------------------------------------\n",
      "loss: 0.754536 [   64/10000]\n",
      "loss: 0.736602 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.698251\n",
      "\n",
      "Epoch 85\n",
      "------------------------------------------------------------------\n",
      "loss: 0.752599 [   64/10000]\n",
      "loss: 0.733821 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.695285\n",
      "\n",
      "Epoch 86\n",
      "------------------------------------------------------------------\n",
      "loss: 0.750674 [   64/10000]\n",
      "loss: 0.731092 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.692360\n",
      "\n",
      "Epoch 87\n",
      "------------------------------------------------------------------\n",
      "loss: 0.748773 [   64/10000]\n",
      "loss: 0.728404 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.689473\n",
      "\n",
      "Epoch 88\n",
      "------------------------------------------------------------------\n",
      "loss: 0.746891 [   64/10000]\n",
      "loss: 0.725774 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.686625\n",
      "\n",
      "Epoch 89\n",
      "------------------------------------------------------------------\n",
      "loss: 0.745025 [   64/10000]\n",
      "loss: 0.723194 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.683811\n",
      "\n",
      "Epoch 90\n",
      "------------------------------------------------------------------\n",
      "loss: 0.743164 [   64/10000]\n",
      "loss: 0.720656 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.681030\n",
      "\n",
      "Epoch 91\n",
      "------------------------------------------------------------------\n",
      "loss: 0.741286 [   64/10000]\n",
      "loss: 0.718145 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.678287\n",
      "\n",
      "Epoch 92\n",
      "------------------------------------------------------------------\n",
      "loss: 0.739420 [   64/10000]\n",
      "loss: 0.715683 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.675588\n",
      "\n",
      "Epoch 93\n",
      "------------------------------------------------------------------\n",
      "loss: 0.737606 [   64/10000]\n",
      "loss: 0.713267 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 76.1%, Avg loss: 0.672929\n",
      "\n",
      "Epoch 94\n",
      "------------------------------------------------------------------\n",
      "loss: 0.735802 [   64/10000]\n",
      "loss: 0.710904 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.670309\n",
      "\n",
      "Epoch 95\n",
      "------------------------------------------------------------------\n",
      "loss: 0.734003 [   64/10000]\n",
      "loss: 0.708575 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.667723\n",
      "\n",
      "Epoch 96\n",
      "------------------------------------------------------------------\n",
      "loss: 0.732219 [   64/10000]\n",
      "loss: 0.706282 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 76.5%, Avg loss: 0.665170\n",
      "\n",
      "Epoch 97\n",
      "------------------------------------------------------------------\n",
      "loss: 0.730443 [   64/10000]\n",
      "loss: 0.704014 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.662650\n",
      "\n",
      "Epoch 98\n",
      "------------------------------------------------------------------\n",
      "loss: 0.728676 [   64/10000]\n",
      "loss: 0.701781 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.660162\n",
      "\n",
      "Epoch 99\n",
      "------------------------------------------------------------------\n",
      "loss: 0.726901 [   64/10000]\n",
      "loss: 0.699584 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 76.8%, Avg loss: 0.657705\n",
      "\n",
      "Epoch 100\n",
      "------------------------------------------------------------------\n",
      "loss: 0.725149 [   64/10000]\n",
      "loss: 0.697426 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.655280\n",
      "\n",
      "Epoch 101\n",
      "------------------------------------------------------------------\n",
      "loss: 0.723399 [   64/10000]\n",
      "loss: 0.695308 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 77.2%, Avg loss: 0.652884\n",
      "\n",
      "Epoch 102\n",
      "------------------------------------------------------------------\n",
      "loss: 0.721660 [   64/10000]\n",
      "loss: 0.693228 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.650518\n",
      "\n",
      "Epoch 103\n",
      "------------------------------------------------------------------\n",
      "loss: 0.719931 [   64/10000]\n",
      "loss: 0.691182 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.648181\n",
      "\n",
      "Epoch 104\n",
      "------------------------------------------------------------------\n",
      "loss: 0.718211 [   64/10000]\n",
      "loss: 0.689172 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 77.5%, Avg loss: 0.645874\n",
      "\n",
      "Epoch 105\n",
      "------------------------------------------------------------------\n",
      "loss: 0.716495 [   64/10000]\n",
      "loss: 0.687196 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 77.5%, Avg loss: 0.643596\n",
      "\n",
      "Epoch 106\n",
      "------------------------------------------------------------------\n",
      "loss: 0.714792 [   64/10000]\n",
      "loss: 0.685239 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: 0.641348\n",
      "\n",
      "Epoch 107\n",
      "------------------------------------------------------------------\n",
      "loss: 0.713098 [   64/10000]\n",
      "loss: 0.683307 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.639127\n",
      "\n",
      "Epoch 108\n",
      "------------------------------------------------------------------\n",
      "loss: 0.711411 [   64/10000]\n",
      "loss: 0.681412 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 77.8%, Avg loss: 0.636936\n",
      "\n",
      "Epoch 109\n",
      "------------------------------------------------------------------\n",
      "loss: 0.709737 [   64/10000]\n",
      "loss: 0.679538 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 77.8%, Avg loss: 0.634772\n",
      "\n",
      "Epoch 110\n",
      "------------------------------------------------------------------\n",
      "loss: 0.708061 [   64/10000]\n",
      "loss: 0.677702 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 77.9%, Avg loss: 0.632636\n",
      "\n",
      "Epoch 111\n",
      "------------------------------------------------------------------\n",
      "loss: 0.706406 [   64/10000]\n",
      "loss: 0.675895 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 78.0%, Avg loss: 0.630527\n",
      "\n",
      "Epoch 112\n",
      "------------------------------------------------------------------\n",
      "loss: 0.704759 [   64/10000]\n",
      "loss: 0.674118 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.628446\n",
      "\n",
      "Epoch 113\n",
      "------------------------------------------------------------------\n",
      "loss: 0.703124 [   64/10000]\n",
      "loss: 0.672360 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.626391\n",
      "\n",
      "Epoch 114\n",
      "------------------------------------------------------------------\n",
      "loss: 0.701492 [   64/10000]\n",
      "loss: 0.670624 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 78.2%, Avg loss: 0.624363\n",
      "\n",
      "Epoch 115\n",
      "------------------------------------------------------------------\n",
      "loss: 0.699875 [   64/10000]\n",
      "loss: 0.668907 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 78.3%, Avg loss: 0.622360\n",
      "\n",
      "Epoch 116\n",
      "------------------------------------------------------------------\n",
      "loss: 0.698269 [   64/10000]\n",
      "loss: 0.667212 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 78.4%, Avg loss: 0.620383\n",
      "\n",
      "Epoch 117\n",
      "------------------------------------------------------------------\n",
      "loss: 0.696674 [   64/10000]\n",
      "loss: 0.665541 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 78.5%, Avg loss: 0.618431\n",
      "\n",
      "Epoch 118\n",
      "------------------------------------------------------------------\n",
      "loss: 0.695083 [   64/10000]\n",
      "loss: 0.663896 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 78.5%, Avg loss: 0.616503\n",
      "\n",
      "Epoch 119\n",
      "------------------------------------------------------------------\n",
      "loss: 0.693500 [   64/10000]\n",
      "loss: 0.662282 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 78.7%, Avg loss: 0.614600\n",
      "\n",
      "Epoch 120\n",
      "------------------------------------------------------------------\n",
      "loss: 0.691938 [   64/10000]\n",
      "loss: 0.660685 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 78.7%, Avg loss: 0.612721\n",
      "\n",
      "Epoch 121\n",
      "------------------------------------------------------------------\n",
      "loss: 0.690377 [   64/10000]\n",
      "loss: 0.659120 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 78.8%, Avg loss: 0.610866\n",
      "\n",
      "Epoch 122\n",
      "------------------------------------------------------------------\n",
      "loss: 0.688824 [   64/10000]\n",
      "loss: 0.657578 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 78.9%, Avg loss: 0.609035\n",
      "\n",
      "Epoch 123\n",
      "------------------------------------------------------------------\n",
      "loss: 0.687287 [   64/10000]\n",
      "loss: 0.656063 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 78.9%, Avg loss: 0.607227\n",
      "\n",
      "Epoch 124\n",
      "------------------------------------------------------------------\n",
      "loss: 0.685761 [   64/10000]\n",
      "loss: 0.654576 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 79.0%, Avg loss: 0.605441\n",
      "\n",
      "Epoch 125\n",
      "------------------------------------------------------------------\n",
      "loss: 0.684243 [   64/10000]\n",
      "loss: 0.653097 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 79.1%, Avg loss: 0.603678\n",
      "\n",
      "Epoch 126\n",
      "------------------------------------------------------------------\n",
      "loss: 0.682720 [   64/10000]\n",
      "loss: 0.651643 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 79.1%, Avg loss: 0.601936\n",
      "\n",
      "Epoch 127\n",
      "------------------------------------------------------------------\n",
      "loss: 0.681217 [   64/10000]\n",
      "loss: 0.650212 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 79.2%, Avg loss: 0.600216\n",
      "\n",
      "Epoch 128\n",
      "------------------------------------------------------------------\n",
      "loss: 0.679716 [   64/10000]\n",
      "loss: 0.648809 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 79.3%, Avg loss: 0.598517\n",
      "\n",
      "Epoch 129\n",
      "------------------------------------------------------------------\n",
      "loss: 0.678222 [   64/10000]\n",
      "loss: 0.647430 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 79.3%, Avg loss: 0.596839\n",
      "\n",
      "Epoch 130\n",
      "------------------------------------------------------------------\n",
      "loss: 0.676733 [   64/10000]\n",
      "loss: 0.646070 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 79.4%, Avg loss: 0.595182\n",
      "\n",
      "Epoch 131\n",
      "------------------------------------------------------------------\n",
      "loss: 0.675260 [   64/10000]\n",
      "loss: 0.644731 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 79.5%, Avg loss: 0.593544\n",
      "\n",
      "Epoch 132\n",
      "------------------------------------------------------------------\n",
      "loss: 0.673795 [   64/10000]\n",
      "loss: 0.643411 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 79.6%, Avg loss: 0.591927\n",
      "\n",
      "Epoch 133\n",
      "------------------------------------------------------------------\n",
      "loss: 0.672341 [   64/10000]\n",
      "loss: 0.642115 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 79.6%, Avg loss: 0.590330\n",
      "\n",
      "Epoch 134\n",
      "------------------------------------------------------------------\n",
      "loss: 0.670898 [   64/10000]\n",
      "loss: 0.640828 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 79.7%, Avg loss: 0.588753\n",
      "\n",
      "Epoch 135\n",
      "------------------------------------------------------------------\n",
      "loss: 0.669464 [   64/10000]\n",
      "loss: 0.639564 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 79.7%, Avg loss: 0.587194\n",
      "\n",
      "Epoch 136\n",
      "------------------------------------------------------------------\n",
      "loss: 0.668036 [   64/10000]\n",
      "loss: 0.638320 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 79.8%, Avg loss: 0.585655\n",
      "\n",
      "Epoch 137\n",
      "------------------------------------------------------------------\n",
      "loss: 0.666612 [   64/10000]\n",
      "loss: 0.637103 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 79.9%, Avg loss: 0.584134\n",
      "\n",
      "Epoch 138\n",
      "------------------------------------------------------------------\n",
      "loss: 0.665198 [   64/10000]\n",
      "loss: 0.635900 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 79.9%, Avg loss: 0.582633\n",
      "\n",
      "Epoch 139\n",
      "------------------------------------------------------------------\n",
      "loss: 0.663790 [   64/10000]\n",
      "loss: 0.634703 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 80.0%, Avg loss: 0.581148\n",
      "\n",
      "Epoch 140\n",
      "------------------------------------------------------------------\n",
      "loss: 0.662386 [   64/10000]\n",
      "loss: 0.633536 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 80.0%, Avg loss: 0.579682\n",
      "\n",
      "Epoch 141\n",
      "------------------------------------------------------------------\n",
      "loss: 0.660991 [   64/10000]\n",
      "loss: 0.632378 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 80.0%, Avg loss: 0.578233\n",
      "\n",
      "Epoch 142\n",
      "------------------------------------------------------------------\n",
      "loss: 0.659606 [   64/10000]\n",
      "loss: 0.631235 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 80.1%, Avg loss: 0.576801\n",
      "\n",
      "Epoch 143\n",
      "------------------------------------------------------------------\n",
      "loss: 0.658223 [   64/10000]\n",
      "loss: 0.630107 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 80.1%, Avg loss: 0.575386\n",
      "\n",
      "Epoch 144\n",
      "------------------------------------------------------------------\n",
      "loss: 0.656844 [   64/10000]\n",
      "loss: 0.628997 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 80.1%, Avg loss: 0.573987\n",
      "\n",
      "Epoch 145\n",
      "------------------------------------------------------------------\n",
      "loss: 0.655486 [   64/10000]\n",
      "loss: 0.627904 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.572605\n",
      "\n",
      "Epoch 146\n",
      "------------------------------------------------------------------\n",
      "loss: 0.654132 [   64/10000]\n",
      "loss: 0.626822 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.571238\n",
      "\n",
      "Epoch 147\n",
      "------------------------------------------------------------------\n",
      "loss: 0.652789 [   64/10000]\n",
      "loss: 0.625755 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.569887\n",
      "\n",
      "Epoch 148\n",
      "------------------------------------------------------------------\n",
      "loss: 0.651446 [   64/10000]\n",
      "loss: 0.624711 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 80.3%, Avg loss: 0.568551\n",
      "\n",
      "Epoch 149\n",
      "------------------------------------------------------------------\n",
      "loss: 0.650110 [   64/10000]\n",
      "loss: 0.623683 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 80.3%, Avg loss: 0.567230\n",
      "\n",
      "Epoch 150\n",
      "------------------------------------------------------------------\n",
      "loss: 0.648777 [   64/10000]\n",
      "loss: 0.622676 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 80.4%, Avg loss: 0.565924\n",
      "\n",
      "Epoch 151\n",
      "------------------------------------------------------------------\n",
      "loss: 0.647449 [   64/10000]\n",
      "loss: 0.621685 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 80.4%, Avg loss: 0.564634\n",
      "\n",
      "Epoch 152\n",
      "------------------------------------------------------------------\n",
      "loss: 0.646121 [   64/10000]\n",
      "loss: 0.620700 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 80.5%, Avg loss: 0.563357\n",
      "\n",
      "Epoch 153\n",
      "------------------------------------------------------------------\n",
      "loss: 0.644806 [   64/10000]\n",
      "loss: 0.619733 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 80.5%, Avg loss: 0.562095\n",
      "\n",
      "Epoch 154\n",
      "------------------------------------------------------------------\n",
      "loss: 0.643490 [   64/10000]\n",
      "loss: 0.618784 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 80.6%, Avg loss: 0.560847\n",
      "\n",
      "Epoch 155\n",
      "------------------------------------------------------------------\n",
      "loss: 0.642198 [   64/10000]\n",
      "loss: 0.617845 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 80.7%, Avg loss: 0.559614\n",
      "\n",
      "Epoch 156\n",
      "------------------------------------------------------------------\n",
      "loss: 0.640912 [   64/10000]\n",
      "loss: 0.616908 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 80.7%, Avg loss: 0.558394\n",
      "\n",
      "Epoch 157\n",
      "------------------------------------------------------------------\n",
      "loss: 0.639625 [   64/10000]\n",
      "loss: 0.615985 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 80.8%, Avg loss: 0.557188\n",
      "\n",
      "Epoch 158\n",
      "------------------------------------------------------------------\n",
      "loss: 0.638346 [   64/10000]\n",
      "loss: 0.615073 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 80.8%, Avg loss: 0.555994\n",
      "\n",
      "Epoch 159\n",
      "------------------------------------------------------------------\n",
      "loss: 0.637066 [   64/10000]\n",
      "loss: 0.614168 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 80.9%, Avg loss: 0.554814\n",
      "\n",
      "Epoch 160\n",
      "------------------------------------------------------------------\n",
      "loss: 0.635802 [   64/10000]\n",
      "loss: 0.613275 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 80.9%, Avg loss: 0.553645\n",
      "\n",
      "Epoch 161\n",
      "------------------------------------------------------------------\n",
      "loss: 0.634542 [   64/10000]\n",
      "loss: 0.612388 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 81.0%, Avg loss: 0.552488\n",
      "\n",
      "Epoch 162\n",
      "------------------------------------------------------------------\n",
      "loss: 0.633276 [   64/10000]\n",
      "loss: 0.611528 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 81.0%, Avg loss: 0.551345\n",
      "\n",
      "Epoch 163\n",
      "------------------------------------------------------------------\n",
      "loss: 0.632036 [   64/10000]\n",
      "loss: 0.610678 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 81.0%, Avg loss: 0.550213\n",
      "\n",
      "Epoch 164\n",
      "------------------------------------------------------------------\n",
      "loss: 0.630801 [   64/10000]\n",
      "loss: 0.609831 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 81.1%, Avg loss: 0.549094\n",
      "\n",
      "Epoch 165\n",
      "------------------------------------------------------------------\n",
      "loss: 0.629573 [   64/10000]\n",
      "loss: 0.608986 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 81.1%, Avg loss: 0.547986\n",
      "\n",
      "Epoch 166\n",
      "------------------------------------------------------------------\n",
      "loss: 0.628351 [   64/10000]\n",
      "loss: 0.608156 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 81.1%, Avg loss: 0.546892\n",
      "\n",
      "Epoch 167\n",
      "------------------------------------------------------------------\n",
      "loss: 0.627147 [   64/10000]\n",
      "loss: 0.607331 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 81.2%, Avg loss: 0.545808\n",
      "\n",
      "Epoch 168\n",
      "------------------------------------------------------------------\n",
      "loss: 0.625950 [   64/10000]\n",
      "loss: 0.606518 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 81.2%, Avg loss: 0.544736\n",
      "\n",
      "Epoch 169\n",
      "------------------------------------------------------------------\n",
      "loss: 0.624754 [   64/10000]\n",
      "loss: 0.605722 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 81.2%, Avg loss: 0.543676\n",
      "\n",
      "Epoch 170\n",
      "------------------------------------------------------------------\n",
      "loss: 0.623573 [   64/10000]\n",
      "loss: 0.604933 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 81.2%, Avg loss: 0.542627\n",
      "\n",
      "Epoch 171\n",
      "------------------------------------------------------------------\n",
      "loss: 0.622397 [   64/10000]\n",
      "loss: 0.604148 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 81.3%, Avg loss: 0.541589\n",
      "\n",
      "Epoch 172\n",
      "------------------------------------------------------------------\n",
      "loss: 0.621232 [   64/10000]\n",
      "loss: 0.603375 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 81.3%, Avg loss: 0.540562\n",
      "\n",
      "Epoch 173\n",
      "------------------------------------------------------------------\n",
      "loss: 0.620069 [   64/10000]\n",
      "loss: 0.602599 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 81.3%, Avg loss: 0.539546\n",
      "\n",
      "Epoch 174\n",
      "------------------------------------------------------------------\n",
      "loss: 0.618902 [   64/10000]\n",
      "loss: 0.601833 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 81.3%, Avg loss: 0.538540\n",
      "\n",
      "Epoch 175\n",
      "------------------------------------------------------------------\n",
      "loss: 0.617763 [   64/10000]\n",
      "loss: 0.601075 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 81.4%, Avg loss: 0.537544\n",
      "\n",
      "Epoch 176\n",
      "------------------------------------------------------------------\n",
      "loss: 0.616617 [   64/10000]\n",
      "loss: 0.600328 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 81.4%, Avg loss: 0.536559\n",
      "\n",
      "Epoch 177\n",
      "------------------------------------------------------------------\n",
      "loss: 0.615471 [   64/10000]\n",
      "loss: 0.599596 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.535584\n",
      "\n",
      "Epoch 178\n",
      "------------------------------------------------------------------\n",
      "loss: 0.614355 [   64/10000]\n",
      "loss: 0.598863 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.534618\n",
      "\n",
      "Epoch 179\n",
      "------------------------------------------------------------------\n",
      "loss: 0.613223 [   64/10000]\n",
      "loss: 0.598139 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.533661\n",
      "\n",
      "Epoch 180\n",
      "------------------------------------------------------------------\n",
      "loss: 0.612108 [   64/10000]\n",
      "loss: 0.597419 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.532715\n",
      "\n",
      "Epoch 181\n",
      "------------------------------------------------------------------\n",
      "loss: 0.610992 [   64/10000]\n",
      "loss: 0.596706 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.531778\n",
      "\n",
      "Epoch 182\n",
      "------------------------------------------------------------------\n",
      "loss: 0.609897 [   64/10000]\n",
      "loss: 0.596004 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.530850\n",
      "\n",
      "Epoch 183\n",
      "------------------------------------------------------------------\n",
      "loss: 0.608804 [   64/10000]\n",
      "loss: 0.595311 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.529930\n",
      "\n",
      "Epoch 184\n",
      "------------------------------------------------------------------\n",
      "loss: 0.607713 [   64/10000]\n",
      "loss: 0.594621 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 81.6%, Avg loss: 0.529020\n",
      "\n",
      "Epoch 185\n",
      "------------------------------------------------------------------\n",
      "loss: 0.606634 [   64/10000]\n",
      "loss: 0.593935 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 81.7%, Avg loss: 0.528118\n",
      "\n",
      "Epoch 186\n",
      "------------------------------------------------------------------\n",
      "loss: 0.605553 [   64/10000]\n",
      "loss: 0.593262 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 81.7%, Avg loss: 0.527225\n",
      "\n",
      "Epoch 187\n",
      "------------------------------------------------------------------\n",
      "loss: 0.604492 [   64/10000]\n",
      "loss: 0.592597 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 81.7%, Avg loss: 0.526340\n",
      "\n",
      "Epoch 188\n",
      "------------------------------------------------------------------\n",
      "loss: 0.603431 [   64/10000]\n",
      "loss: 0.591921 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 81.8%, Avg loss: 0.525463\n",
      "\n",
      "Epoch 189\n",
      "------------------------------------------------------------------\n",
      "loss: 0.602373 [   64/10000]\n",
      "loss: 0.591256 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 81.8%, Avg loss: 0.524594\n",
      "\n",
      "Epoch 190\n",
      "------------------------------------------------------------------\n",
      "loss: 0.601327 [   64/10000]\n",
      "loss: 0.590596 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 81.8%, Avg loss: 0.523732\n",
      "\n",
      "Epoch 191\n",
      "------------------------------------------------------------------\n",
      "loss: 0.600281 [   64/10000]\n",
      "loss: 0.589930 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.522878\n",
      "\n",
      "Epoch 192\n",
      "------------------------------------------------------------------\n",
      "loss: 0.599230 [   64/10000]\n",
      "loss: 0.589285 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.522032\n",
      "\n",
      "Epoch 193\n",
      "------------------------------------------------------------------\n",
      "loss: 0.598189 [   64/10000]\n",
      "loss: 0.588640 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.521194\n",
      "\n",
      "Epoch 194\n",
      "------------------------------------------------------------------\n",
      "loss: 0.597162 [   64/10000]\n",
      "loss: 0.588004 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.520363\n",
      "\n",
      "Epoch 195\n",
      "------------------------------------------------------------------\n",
      "loss: 0.596136 [   64/10000]\n",
      "loss: 0.587363 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.519539\n",
      "\n",
      "Epoch 196\n",
      "------------------------------------------------------------------\n",
      "loss: 0.595127 [   64/10000]\n",
      "loss: 0.586733 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.518722\n",
      "\n",
      "Epoch 197\n",
      "------------------------------------------------------------------\n",
      "loss: 0.594115 [   64/10000]\n",
      "loss: 0.586092 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.517913\n",
      "\n",
      "Epoch 198\n",
      "------------------------------------------------------------------\n",
      "loss: 0.593116 [   64/10000]\n",
      "loss: 0.585483 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.517110\n",
      "\n",
      "Epoch 199\n",
      "------------------------------------------------------------------\n",
      "loss: 0.592119 [   64/10000]\n",
      "loss: 0.584870 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.516315\n",
      "\n",
      "Epoch 200\n",
      "------------------------------------------------------------------\n",
      "loss: 0.591121 [   64/10000]\n",
      "loss: 0.584253 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 82.0%, Avg loss: 0.515526\n",
      "\n",
      "Epoch 201\n",
      "------------------------------------------------------------------\n",
      "loss: 0.590131 [   64/10000]\n",
      "loss: 0.583647 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 82.0%, Avg loss: 0.514745\n",
      "\n",
      "Epoch 202\n",
      "------------------------------------------------------------------\n",
      "loss: 0.589154 [   64/10000]\n",
      "loss: 0.583043 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 82.0%, Avg loss: 0.513969\n",
      "\n",
      "Epoch 203\n",
      "------------------------------------------------------------------\n",
      "loss: 0.588168 [   64/10000]\n",
      "loss: 0.582440 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 82.1%, Avg loss: 0.513201\n",
      "\n",
      "Epoch 204\n",
      "------------------------------------------------------------------\n",
      "loss: 0.587203 [   64/10000]\n",
      "loss: 0.581838 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 82.1%, Avg loss: 0.512439\n",
      "\n",
      "Epoch 205\n",
      "------------------------------------------------------------------\n",
      "loss: 0.586242 [   64/10000]\n",
      "loss: 0.581246 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 82.1%, Avg loss: 0.511683\n",
      "\n",
      "Epoch 206\n",
      "------------------------------------------------------------------\n",
      "loss: 0.585282 [   64/10000]\n",
      "loss: 0.580657 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 82.1%, Avg loss: 0.510932\n",
      "\n",
      "Epoch 207\n",
      "------------------------------------------------------------------\n",
      "loss: 0.584320 [   64/10000]\n",
      "loss: 0.580077 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 0.510187\n",
      "\n",
      "Epoch 208\n",
      "------------------------------------------------------------------\n",
      "loss: 0.583353 [   64/10000]\n",
      "loss: 0.579498 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 0.509448\n",
      "\n",
      "Epoch 209\n",
      "------------------------------------------------------------------\n",
      "loss: 0.582400 [   64/10000]\n",
      "loss: 0.578930 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 0.508715\n",
      "\n",
      "Epoch 210\n",
      "------------------------------------------------------------------\n",
      "loss: 0.581471 [   64/10000]\n",
      "loss: 0.578351 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 0.507988\n",
      "\n",
      "Epoch 211\n",
      "------------------------------------------------------------------\n",
      "loss: 0.580541 [   64/10000]\n",
      "loss: 0.577787 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 0.507267\n",
      "\n",
      "Epoch 212\n",
      "------------------------------------------------------------------\n",
      "loss: 0.579609 [   64/10000]\n",
      "loss: 0.577216 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 0.506552\n",
      "\n",
      "Epoch 213\n",
      "------------------------------------------------------------------\n",
      "loss: 0.578675 [   64/10000]\n",
      "loss: 0.576656 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 82.4%, Avg loss: 0.505842\n",
      "\n",
      "Epoch 214\n",
      "------------------------------------------------------------------\n",
      "loss: 0.577752 [   64/10000]\n",
      "loss: 0.576093 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 82.4%, Avg loss: 0.505140\n",
      "\n",
      "Epoch 215\n",
      "------------------------------------------------------------------\n",
      "loss: 0.576840 [   64/10000]\n",
      "loss: 0.575529 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 82.4%, Avg loss: 0.504443\n",
      "\n",
      "Epoch 216\n",
      "------------------------------------------------------------------\n",
      "loss: 0.575928 [   64/10000]\n",
      "loss: 0.574978 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 82.4%, Avg loss: 0.503752\n",
      "\n",
      "Epoch 217\n",
      "------------------------------------------------------------------\n",
      "loss: 0.575023 [   64/10000]\n",
      "loss: 0.574420 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 82.4%, Avg loss: 0.503066\n",
      "\n",
      "Epoch 218\n",
      "------------------------------------------------------------------\n",
      "loss: 0.574135 [   64/10000]\n",
      "loss: 0.573877 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 82.5%, Avg loss: 0.502384\n",
      "\n",
      "Epoch 219\n",
      "------------------------------------------------------------------\n",
      "loss: 0.573239 [   64/10000]\n",
      "loss: 0.573336 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 82.5%, Avg loss: 0.501709\n",
      "\n",
      "Epoch 220\n",
      "------------------------------------------------------------------\n",
      "loss: 0.572356 [   64/10000]\n",
      "loss: 0.572806 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.501038\n",
      "\n",
      "Epoch 221\n",
      "------------------------------------------------------------------\n",
      "loss: 0.571467 [   64/10000]\n",
      "loss: 0.572273 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.500372\n",
      "\n",
      "Epoch 222\n",
      "------------------------------------------------------------------\n",
      "loss: 0.570592 [   64/10000]\n",
      "loss: 0.571747 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.499712\n",
      "\n",
      "Epoch 223\n",
      "------------------------------------------------------------------\n",
      "loss: 0.569712 [   64/10000]\n",
      "loss: 0.571235 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.499056\n",
      "\n",
      "Epoch 224\n",
      "------------------------------------------------------------------\n",
      "loss: 0.568841 [   64/10000]\n",
      "loss: 0.570717 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.498406\n",
      "\n",
      "Epoch 225\n",
      "------------------------------------------------------------------\n",
      "loss: 0.567970 [   64/10000]\n",
      "loss: 0.570193 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.497761\n",
      "\n",
      "Epoch 226\n",
      "------------------------------------------------------------------\n",
      "loss: 0.567102 [   64/10000]\n",
      "loss: 0.569670 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 82.8%, Avg loss: 0.497121\n",
      "\n",
      "Epoch 227\n",
      "------------------------------------------------------------------\n",
      "loss: 0.566234 [   64/10000]\n",
      "loss: 0.569146 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.496485\n",
      "\n",
      "Epoch 228\n",
      "------------------------------------------------------------------\n",
      "loss: 0.565367 [   64/10000]\n",
      "loss: 0.568624 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.495854\n",
      "\n",
      "Epoch 229\n",
      "------------------------------------------------------------------\n",
      "loss: 0.564514 [   64/10000]\n",
      "loss: 0.568109 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.495227\n",
      "\n",
      "Epoch 230\n",
      "------------------------------------------------------------------\n",
      "loss: 0.563647 [   64/10000]\n",
      "loss: 0.567587 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.494605\n",
      "\n",
      "Epoch 231\n",
      "------------------------------------------------------------------\n",
      "loss: 0.562799 [   64/10000]\n",
      "loss: 0.567077 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 82.8%, Avg loss: 0.493988\n",
      "\n",
      "Epoch 232\n",
      "------------------------------------------------------------------\n",
      "loss: 0.561941 [   64/10000]\n",
      "loss: 0.566567 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 82.8%, Avg loss: 0.493375\n",
      "\n",
      "Epoch 233\n",
      "------------------------------------------------------------------\n",
      "loss: 0.561098 [   64/10000]\n",
      "loss: 0.566062 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 82.8%, Avg loss: 0.492767\n",
      "\n",
      "Epoch 234\n",
      "------------------------------------------------------------------\n",
      "loss: 0.560245 [   64/10000]\n",
      "loss: 0.565561 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 82.9%, Avg loss: 0.492162\n",
      "\n",
      "Epoch 235\n",
      "------------------------------------------------------------------\n",
      "loss: 0.559390 [   64/10000]\n",
      "loss: 0.565063 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 82.9%, Avg loss: 0.491562\n",
      "\n",
      "Epoch 236\n",
      "------------------------------------------------------------------\n",
      "loss: 0.558550 [   64/10000]\n",
      "loss: 0.564574 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 82.9%, Avg loss: 0.490966\n",
      "\n",
      "Epoch 237\n",
      "------------------------------------------------------------------\n",
      "loss: 0.557711 [   64/10000]\n",
      "loss: 0.564094 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 82.9%, Avg loss: 0.490374\n",
      "\n",
      "Epoch 238\n",
      "------------------------------------------------------------------\n",
      "loss: 0.556880 [   64/10000]\n",
      "loss: 0.563597 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.489787\n",
      "\n",
      "Epoch 239\n",
      "------------------------------------------------------------------\n",
      "loss: 0.556047 [   64/10000]\n",
      "loss: 0.563105 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.489203\n",
      "\n",
      "Epoch 240\n",
      "------------------------------------------------------------------\n",
      "loss: 0.555222 [   64/10000]\n",
      "loss: 0.562618 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.488623\n",
      "\n",
      "Epoch 241\n",
      "------------------------------------------------------------------\n",
      "loss: 0.554386 [   64/10000]\n",
      "loss: 0.562124 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.488047\n",
      "\n",
      "Epoch 242\n",
      "------------------------------------------------------------------\n",
      "loss: 0.553555 [   64/10000]\n",
      "loss: 0.561636 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.487475\n",
      "\n",
      "Epoch 243\n",
      "------------------------------------------------------------------\n",
      "loss: 0.552727 [   64/10000]\n",
      "loss: 0.561143 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.486907\n",
      "\n",
      "Epoch 244\n",
      "------------------------------------------------------------------\n",
      "loss: 0.551895 [   64/10000]\n",
      "loss: 0.560646 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.486342\n",
      "\n",
      "Epoch 245\n",
      "------------------------------------------------------------------\n",
      "loss: 0.551075 [   64/10000]\n",
      "loss: 0.560147 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.485781\n",
      "\n",
      "Epoch 246\n",
      "------------------------------------------------------------------\n",
      "loss: 0.550252 [   64/10000]\n",
      "loss: 0.559653 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 83.1%, Avg loss: 0.485224\n",
      "\n",
      "Epoch 247\n",
      "------------------------------------------------------------------\n",
      "loss: 0.549437 [   64/10000]\n",
      "loss: 0.559156 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.484670\n",
      "\n",
      "Epoch 248\n",
      "------------------------------------------------------------------\n",
      "loss: 0.548619 [   64/10000]\n",
      "loss: 0.558656 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.484120\n",
      "\n",
      "Epoch 249\n",
      "------------------------------------------------------------------\n",
      "loss: 0.547815 [   64/10000]\n",
      "loss: 0.558153 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.483574\n",
      "\n",
      "Epoch 250\n",
      "------------------------------------------------------------------\n",
      "loss: 0.547006 [   64/10000]\n",
      "loss: 0.557654 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 83.1%, Avg loss: 0.483031\n",
      "\n",
      "Epoch 251\n",
      "------------------------------------------------------------------\n",
      "loss: 0.546216 [   64/10000]\n",
      "loss: 0.557158 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 83.1%, Avg loss: 0.482492\n",
      "\n",
      "Epoch 252\n",
      "------------------------------------------------------------------\n",
      "loss: 0.545410 [   64/10000]\n",
      "loss: 0.556662 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 83.1%, Avg loss: 0.481955\n",
      "\n",
      "Epoch 253\n",
      "------------------------------------------------------------------\n",
      "loss: 0.544610 [   64/10000]\n",
      "loss: 0.556165 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 83.1%, Avg loss: 0.481421\n",
      "\n",
      "Epoch 254\n",
      "------------------------------------------------------------------\n",
      "loss: 0.543806 [   64/10000]\n",
      "loss: 0.555669 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 83.1%, Avg loss: 0.480891\n",
      "\n",
      "Epoch 255\n",
      "------------------------------------------------------------------\n",
      "loss: 0.543016 [   64/10000]\n",
      "loss: 0.555188 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.480363\n",
      "\n",
      "Epoch 256\n",
      "------------------------------------------------------------------\n",
      "loss: 0.542208 [   64/10000]\n",
      "loss: 0.554694 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.479839\n",
      "\n",
      "Epoch 257\n",
      "------------------------------------------------------------------\n",
      "loss: 0.541413 [   64/10000]\n",
      "loss: 0.554208 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.479318\n",
      "\n",
      "Epoch 258\n",
      "------------------------------------------------------------------\n",
      "loss: 0.540628 [   64/10000]\n",
      "loss: 0.553721 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.478800\n",
      "\n",
      "Epoch 259\n",
      "------------------------------------------------------------------\n",
      "loss: 0.539848 [   64/10000]\n",
      "loss: 0.553253 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.478285\n",
      "\n",
      "Epoch 260\n",
      "------------------------------------------------------------------\n",
      "loss: 0.539073 [   64/10000]\n",
      "loss: 0.552776 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.477772\n",
      "\n",
      "Epoch 261\n",
      "------------------------------------------------------------------\n",
      "loss: 0.538289 [   64/10000]\n",
      "loss: 0.552304 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.477263\n",
      "\n",
      "Epoch 262\n",
      "------------------------------------------------------------------\n",
      "loss: 0.537520 [   64/10000]\n",
      "loss: 0.551829 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.476757\n",
      "\n",
      "Epoch 263\n",
      "------------------------------------------------------------------\n",
      "loss: 0.536743 [   64/10000]\n",
      "loss: 0.551351 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.476253\n",
      "\n",
      "Epoch 264\n",
      "------------------------------------------------------------------\n",
      "loss: 0.535974 [   64/10000]\n",
      "loss: 0.550867 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.475752\n",
      "\n",
      "Epoch 265\n",
      "------------------------------------------------------------------\n",
      "loss: 0.535208 [   64/10000]\n",
      "loss: 0.550394 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.475254\n",
      "\n",
      "Epoch 266\n",
      "------------------------------------------------------------------\n",
      "loss: 0.534435 [   64/10000]\n",
      "loss: 0.549918 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 83.4%, Avg loss: 0.474759\n",
      "\n",
      "Epoch 267\n",
      "------------------------------------------------------------------\n",
      "loss: 0.533663 [   64/10000]\n",
      "loss: 0.549450 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 83.4%, Avg loss: 0.474266\n",
      "\n",
      "Epoch 268\n",
      "------------------------------------------------------------------\n",
      "loss: 0.532903 [   64/10000]\n",
      "loss: 0.548981 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 83.5%, Avg loss: 0.473776\n",
      "\n",
      "Epoch 269\n",
      "------------------------------------------------------------------\n",
      "loss: 0.532144 [   64/10000]\n",
      "loss: 0.548507 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 83.5%, Avg loss: 0.473289\n",
      "\n",
      "Epoch 270\n",
      "------------------------------------------------------------------\n",
      "loss: 0.531379 [   64/10000]\n",
      "loss: 0.548037 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 83.5%, Avg loss: 0.472804\n",
      "\n",
      "Epoch 271\n",
      "------------------------------------------------------------------\n",
      "loss: 0.530599 [   64/10000]\n",
      "loss: 0.547562 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 83.5%, Avg loss: 0.472321\n",
      "\n",
      "Epoch 272\n",
      "------------------------------------------------------------------\n",
      "loss: 0.529826 [   64/10000]\n",
      "loss: 0.547096 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 83.5%, Avg loss: 0.471840\n",
      "\n",
      "Epoch 273\n",
      "------------------------------------------------------------------\n",
      "loss: 0.529063 [   64/10000]\n",
      "loss: 0.546618 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 83.5%, Avg loss: 0.471362\n",
      "\n",
      "Epoch 274\n",
      "------------------------------------------------------------------\n",
      "loss: 0.528308 [   64/10000]\n",
      "loss: 0.546166 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 83.5%, Avg loss: 0.470886\n",
      "\n",
      "Epoch 275\n",
      "------------------------------------------------------------------\n",
      "loss: 0.527565 [   64/10000]\n",
      "loss: 0.545700 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 83.5%, Avg loss: 0.470412\n",
      "\n",
      "Epoch 276\n",
      "------------------------------------------------------------------\n",
      "loss: 0.526816 [   64/10000]\n",
      "loss: 0.545227 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 83.5%, Avg loss: 0.469941\n",
      "\n",
      "Epoch 277\n",
      "------------------------------------------------------------------\n",
      "loss: 0.526054 [   64/10000]\n",
      "loss: 0.544760 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 83.6%, Avg loss: 0.469472\n",
      "\n",
      "Epoch 278\n",
      "------------------------------------------------------------------\n",
      "loss: 0.525316 [   64/10000]\n",
      "loss: 0.544293 [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 83.6%, Avg loss: 0.469006\n",
      "\n",
      "Epoch 279\n",
      "------------------------------------------------------------------\n",
      "loss: 0.524560 [   64/10000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[11]\u001B[39m\u001B[32m, line 102\u001B[39m\n\u001B[32m    100\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[32m    101\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mt\u001B[38;5;250m \u001B[39m+\u001B[38;5;250m \u001B[39m\u001B[32m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m------------------------------------------------------------------\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m102\u001B[39m     \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    103\u001B[39m     test(test_dataloader, model, loss_fn)\n\u001B[32m    104\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mDone!\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[11]\u001B[39m\u001B[32m, line 68\u001B[39m, in \u001B[36mtrain\u001B[39m\u001B[34m(dataloader, model, loss_fn, optimizer)\u001B[39m\n\u001B[32m     66\u001B[39m size = \u001B[38;5;28mlen\u001B[39m(dataloader.dataset)\n\u001B[32m     67\u001B[39m model.train()\n\u001B[32m---> \u001B[39m\u001B[32m68\u001B[39m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdataloader\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m     69\u001B[39m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     71\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpred\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/Hello-World/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001B[39m, in \u001B[36m_BaseDataLoaderIter.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    730\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    731\u001B[39m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[32m    732\u001B[39m     \u001B[38;5;28mself\u001B[39m._reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m733\u001B[39m data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    734\u001B[39m \u001B[38;5;28mself\u001B[39m._num_yielded += \u001B[32m1\u001B[39m\n\u001B[32m    735\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    736\u001B[39m     \u001B[38;5;28mself\u001B[39m._dataset_kind == _DatasetKind.Iterable\n\u001B[32m    737\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    738\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._num_yielded > \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called\n\u001B[32m    739\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/Hello-World/lib/python3.12/site-packages/torch/utils/data/dataloader.py:789\u001B[39m, in \u001B[36m_SingleProcessDataLoaderIter._next_data\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    787\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    788\u001B[39m     index = \u001B[38;5;28mself\u001B[39m._next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m789\u001B[39m     data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m    790\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._pin_memory:\n\u001B[32m    791\u001B[39m         data = _utils.pin_memory.pin_memory(data, \u001B[38;5;28mself\u001B[39m._pin_memory_device)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/Hello-World/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001B[39m, in \u001B[36m_MapDatasetFetcher.fetch\u001B[39m\u001B[34m(self, possibly_batched_index)\u001B[39m\n\u001B[32m     50\u001B[39m         data = \u001B[38;5;28mself\u001B[39m.dataset.__getitems__(possibly_batched_index)\n\u001B[32m     51\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m52\u001B[39m         data = [\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[32m     53\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     54\u001B[39m     data = \u001B[38;5;28mself\u001B[39m.dataset[possibly_batched_index]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/Hello-World/lib/python3.12/site-packages/torchvision/datasets/mnist.py:143\u001B[39m, in \u001B[36mMNIST.__getitem__\u001B[39m\u001B[34m(self, index)\u001B[39m\n\u001B[32m    139\u001B[39m img, target = \u001B[38;5;28mself\u001B[39m.data[index], \u001B[38;5;28mint\u001B[39m(\u001B[38;5;28mself\u001B[39m.targets[index])\n\u001B[32m    141\u001B[39m \u001B[38;5;66;03m# doing this so that it is consistent with all other datasets\u001B[39;00m\n\u001B[32m    142\u001B[39m \u001B[38;5;66;03m# to return a PIL Image\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m143\u001B[39m img = \u001B[43mImage\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfromarray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m.\u001B[49m\u001B[43mnumpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mL\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    145\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.transform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    146\u001B[39m     img = \u001B[38;5;28mself\u001B[39m.transform(img)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/Hello-World/lib/python3.12/site-packages/PIL/Image.py:3326\u001B[39m, in \u001B[36mfromarray\u001B[39m\u001B[34m(obj, mode)\u001B[39m\n\u001B[32m   3323\u001B[39m         msg = \u001B[33m\"\u001B[39m\u001B[33m'\u001B[39m\u001B[33mstrides\u001B[39m\u001B[33m'\u001B[39m\u001B[33m requires either tobytes() or tostring()\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   3324\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg)\n\u001B[32m-> \u001B[39m\u001B[32m3326\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfrombuffer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mraw\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrawmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/Hello-World/lib/python3.12/site-packages/PIL/Image.py:3207\u001B[39m, in \u001B[36mfrombuffer\u001B[39m\u001B[34m(mode, size, data, decoder_name, *args)\u001B[39m\n\u001B[32m   3205\u001B[39m     args = mode, \u001B[32m0\u001B[39m, \u001B[32m1\u001B[39m\n\u001B[32m   3206\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m args[\u001B[32m0\u001B[39m] \u001B[38;5;129;01min\u001B[39;00m _MAPMODES:\n\u001B[32m-> \u001B[39m\u001B[32m3207\u001B[39m     im = \u001B[43mnew\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3208\u001B[39m     im = im._new(core.map_buffer(data, size, decoder_name, \u001B[32m0\u001B[39m, args))\n\u001B[32m   3209\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m mode == \u001B[33m\"\u001B[39m\u001B[33mP\u001B[39m\u001B[33m\"\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/Hello-World/lib/python3.12/site-packages/PIL/Image.py:3082\u001B[39m, in \u001B[36mnew\u001B[39m\u001B[34m(mode, size, color)\u001B[39m\n\u001B[32m   3079\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m mode \u001B[38;5;129;01min\u001B[39;00m (\u001B[33m\"\u001B[39m\u001B[33mBGR;15\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mBGR;16\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mBGR;24\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m   3080\u001B[39m     deprecate(mode, \u001B[32m12\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m3082\u001B[39m \u001B[43m_check_size\u001B[49m\u001B[43m(\u001B[49m\u001B[43msize\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3084\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m color \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   3085\u001B[39m     \u001B[38;5;66;03m# don't initialize\u001B[39;00m\n\u001B[32m   3086\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m Image()._new(core.new(mode, size))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/Hello-World/lib/python3.12/site-packages/PIL/Image.py:3040\u001B[39m, in \u001B[36m_check_size\u001B[39m\u001B[34m(size)\u001B[39m\n\u001B[32m   3033\u001B[39m         \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[32m   3036\u001B[39m \u001B[38;5;66;03m# --------------------------------------------------------------------\u001B[39;00m\n\u001B[32m   3037\u001B[39m \u001B[38;5;66;03m# Factories\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m3040\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_check_size\u001B[39m(size: Any) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   3041\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   3042\u001B[39m \u001B[33;03m    Common check to enforce type and sanity check on size tuples\u001B[39;00m\n\u001B[32m   3043\u001B[39m \n\u001B[32m   3044\u001B[39m \u001B[33;03m    :param size: Should be a 2 tuple of (width, height)\u001B[39;00m\n\u001B[32m   3045\u001B[39m \u001B[33;03m    :returns: None, or raises a ValueError\u001B[39;00m\n\u001B[32m   3046\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m   3048\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(size, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
